{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f013060",
   "metadata": {},
   "source": [
    "Building the base LLM Model which will be then used for finetuning LoRA + Quantization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3aedda",
   "metadata": {},
   "source": [
    "choosing base model to do our job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068bc6c6",
   "metadata": {},
   "source": [
    "what is the criteria of selection of a base LLM?\n",
    "- we are going to start with Meta's llama 3.2 1B Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cc357",
   "metadata": {},
   "source": [
    "## Loading Models and Basic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d22a5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karnavivek/askmyprofession/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1b-Instruct\"\n",
    "device = 'mps' # Use 'cuda' for GPU, 'mps' for Mac, or 'cpu' for CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bddd7",
   "metadata": {},
   "source": [
    "before going to the next step, make sure you are logged in through huggingface in web & have created \"access token\", only when you do that, we can access authorized models from the website.\n",
    "\n",
    "after generating & copying access token from huggingface website,\n",
    "go to vscode terminal & type -> \"huggingface-cli login\"\n",
    "\n",
    "then paste your access token,\n",
    "\n",
    "then you can run the below code! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b4a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token as EOS token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=device)\n",
    "#took 48.2s to download the model and load llama 3.2 1b instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2874cfc",
   "metadata": {},
   "source": [
    "CausalLM predict the NEXT WORD given a prefix of sentence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b28e0d",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0a74a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the capital of India? New Delhi?\\nYes, that is correct! New Delhi is indeed the capital of India. I should have been more specific in'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_pipeline = pipeline(task='text-generation',\n",
    "                             model=model,\n",
    "                             tokenizer=tokenizer)\n",
    "\n",
    "generate_pipeline(\"What is the capital of India?\", max_new_tokens=25)\n",
    "#max_new_tokens = number of words to be generated in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e1243",
   "metadata": {},
   "source": [
    "##### Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b91f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"What is the capital of India? New Delhi?\\nYes, that's correct! New Delhi is indeed the capital of India. It's a city located in the\"}],\n",
       " [{'generated_text': 'What is the capital of USA? Washington\\nWhat is the capital of the United States?\\nWashington, D.C. (short for District of Columbia) is the'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_pipeline([\"What is the capital of India?\", \n",
    "                   \"What is the capital of USA?\"], max_new_tokens=25)\n",
    "#If list of sentences is given, The max_new_tokens is applied to each sentence in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044d6f8",
   "metadata": {},
   "source": [
    "### What is happening inside the Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d2342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   3923,    374,    279,   6864,    315,   6890,     30],\n",
      "        [128000,   3923,    374,    279,   6864,    315,   7427,     30]],\n",
      "       device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "#without padding\n",
    "\n",
    "input_prompt = [\"What is the capital of India?\",\n",
    "                \"What is the capital of USA?\"]\n",
    "\n",
    "#tokeinzers convert the input string into list of integers that would be used as input to the model\n",
    "tokenized = tokenizer(input_prompt, return_tensors='pt').to(device)\n",
    "'''\n",
    "return_tensors = 'pt' -> returns PyTorch tensors\n",
    "return_tensors = 'tf' -> returns TensorFlow tensors\n",
    "return_tensors = 'np' -> returns NumPy arrays\n",
    "return_tensors = None -> returns list of python integers\n",
    "'''\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e7dac",
   "metadata": {},
   "source": [
    "Both of the input sentences has been converted into a list of integers & the shape of the list is same, hence we got the results, if the length of the input string would have been different, it could not convert, hence we need to use padding, so when there are missing integers, the tokenizer can fill in placeholders to let the matrix be of same shape & work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f10777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca4fff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128009, 128009, 128000,   3923,    374,    279,   6864,    315,   6890,\n",
      "             30],\n",
      "        [128000,   3923,    374,    279,   6864,    315,   7427,    323,   7008,\n",
      "             30]], device='mps:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "#with padding\n",
    "\n",
    "input_prompt = [\"What is the capital of India?\",\n",
    "                \"What is the capital of USA and Canada?\"]\n",
    "\n",
    "#tokeinzers convert the input string into list of integers that would be used as input to the model\n",
    "tokenized = tokenizer(input_prompt, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a9313dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745b319",
   "metadata": {},
   "source": [
    "From the above demostration, as we have used padding, & as the 2nd string is longer, the 1st string need some padding in order to fill the missing tokens, hence the EOS are addes to the 'left' because we mentioned it at the top, while we were defining tokeizer.\n",
    "we can also see that the shape of the inputs have also changed\n",
    "\n",
    "Now, lets see convert the integer back to string to understand how letter look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1340f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|eot_id|><|eot_id|><|begin_of_text|>What is the capital of India?',\n",
       " '<|begin_of_text|>What is the capital of USA and Canada?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized['input_ids']) #this will convert the list of integers back to string in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70639046",
   "metadata": {},
   "source": [
    "Padding helped the first sentence to fill in the empty numbers, this helps to run the code faster & to a better matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5283af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128009, 128009, 128000,   3923,    374,    279,   6864,    315,   6890,\n",
       "             30],\n",
       "        [128000,   3923,    374,    279,   6864,    315,   7427,    323,   7008,\n",
       "             30]], device='mps:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99bb77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7eaee4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f75bb8",
   "metadata": {},
   "source": [
    "This attention_mask tells the model to NOT give attention to the padding words, here, as we saw, 2 EOS tokes were added to the first sentence because of padding, these will not be considered by the model. the 1's will be counted but not the 0's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ad80b",
   "metadata": {},
   "source": [
    "#### Chat Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc26e3",
   "metadata": {},
   "source": [
    "Instruction Tuning: Many language models are fine tuned to follow user instructions in a chat-like format\n",
    "\n",
    "Hence, we are going to use \"apply_chat_templates()\" which converts prompt from the chat message format to a single string sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee16dd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
       "            264,   7941,  15592,  18328,    889,  21881,   1093,  42482,     13,\n",
       "         128009, 128006,    882, 128007,    271,   2940,   1587,    279,   7160,\n",
       "            743,     30, 128009, 128006,  78191, 128007,    271]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a smart AI assistant who speaks like Shakespeare.\"\n",
    "     },\n",
    "     {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"where does the sun set?\" \n",
    "     }\n",
    "]\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(prompt, \n",
    "                                          add_generation_prompt = True,\n",
    "                                          tokenize = True, #true if you want to convert to list of integers, False if you want string output\n",
    "                                          padding = True,\n",
    "                                          return_tensors = 'pt').to(device)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37552488",
   "metadata": {},
   "source": [
    "Output when the tokenize is set to \"False\":\n",
    "\n",
    "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a smart AI assistant who speaks like Shakespeare.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere does the sun set?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e769db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a smart AI assistant who speaks like Shakespeare.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere does the sun set?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nFair mortal, thou dost ask a query most divine,\\nConcerning the setting of the sun's celestial\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=20)\n",
    "\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdec51de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
       "            264,   7941,  15592,  18328,    889,  21881,   1093,  42482,     13,\n",
       "         128009, 128006,    882, 128007,    271,   2940,   1587,    279,   7160,\n",
       "            743,     30, 128009, 128006,  78191, 128007,    271,   5159,  10457,\n",
       "            713, 128009]], device='mps:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a smart AI assistant who speaks like Shakespeare.\"\n",
    "     },\n",
    "     {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"where does the sun set?\" \n",
    "     },\n",
    "     {\n",
    "         \"role\": \"assistant\",\n",
    "         \"content\": \"My liege\" #sometimes, we want our assistant to start from a specific word\n",
    "     }\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized = tokenizer.apply_chat_template(prompt,\n",
    "                                          add_generation_prompt = False, #prompts the assitant to respond. If False, the model will not generate any output\n",
    "                                          continue_final_message = True, #continues from the last word of the assistant message, basically forces the LM to generate sentence that begin with assitant message\n",
    "                                          tokenize = True, #if this is False, you cant to .to(device) as it is a string\n",
    "                                          padding = True,\n",
    "                                          return_tensors = 'pt').to(device)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0603ae1",
   "metadata": {},
   "source": [
    "- at __add_generation_prompt = True__ -> '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a smart AI assistant who speaks like Shakespeare.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere does the sun set?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMy liege<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "\n",
    "- at __add_generation_prompt = False__ -> '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a smart AI assistant who speaks like Shakespeare.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere does the sun set?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMy liege<|eot_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aada63b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a smart AI assistant who speaks like Shakespeare.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere does the sun set?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMy liege<|eot_id|>'\\n\\nThe sun doth set where the horizon meeteth the sky,\\nA fiery ball of glory,\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=20)\n",
    "\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "decoded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d24c5",
   "metadata": {},
   "source": [
    "### Next Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4876452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "# here we are trying to predict the next word after the sentence \"Hello how are\"\n",
    "\n",
    "text = \"Hello how are\"\n",
    "input_ids = tokenizer([text], return_tensors='pt')['input_ids'].to(device)\n",
    "out = model(input_ids = input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a2ef8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   9906,   1268,    527]], device='mps:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abc032c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 2.8438,  3.5781,  7.0312,  ..., -1.2422, -1.2422, -1.2422],\n",
       "         [19.1250,  3.7812,  3.7031,  ..., -1.1016, -1.1016, -1.1016],\n",
       "         [ 9.3750,  5.8750,  4.0000,  ..., -0.2344, -0.2344, -0.2344],\n",
       "         [ 9.9375,  6.3125,  1.7188,  ...,  0.4160,  0.4160,  0.4160]]],\n",
       "       device='mps:0', grad_fn=<ToCopyBackward0>), past_key_values=((tensor([[[[ 0.1758,  0.1270, -0.0110,  ..., -1.4219,  1.2344, -0.4062],\n",
       "          [-0.4219,  0.4062, -1.0312,  ...,  1.4844, -1.1875, -1.8047],\n",
       "          [-4.7812, -1.5469, -2.4688,  ...,  3.1406, -2.4062, -2.8906],\n",
       "          [-4.7188, -2.7812, -2.8438,  ...,  2.1406, -1.8125, -0.5117]],\n",
       "\n",
       "         [[ 0.0060, -0.0552,  0.0352,  ...,  1.1953, -0.1787, -0.0236],\n",
       "          [ 2.5625, -2.3125,  2.2969,  ...,  2.0000,  2.0938, -0.7188],\n",
       "          [ 2.0938, -0.9141,  3.7500,  ..., -0.2441,  1.3047, -3.2656],\n",
       "          [ 0.0874, -0.2373,  0.3750,  ..., -0.1367,  1.3906, -0.9141]],\n",
       "\n",
       "         [[ 0.0522, -0.0364, -0.0083,  ...,  1.1094,  2.5312, -1.4062],\n",
       "          [ 0.6797, -1.1797, -1.0000,  ..., -0.1113,  0.9531, -0.1836],\n",
       "          [-0.3926, -0.3086, -1.1328,  ..., -0.0192, -0.3223,  1.8750],\n",
       "          [-0.3027, -0.2930, -0.2305,  ..., -0.2373, -0.7617,  1.1953]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0654,  0.1006, -0.0325,  ...,  0.9375, -1.4453,  0.9727],\n",
       "          [-1.7188, -0.0898, -1.0391,  ...,  0.3398,  0.3398, -1.6328],\n",
       "          [ 0.9375, -1.0703, -0.0879,  ..., -0.5234, -0.3594, -1.3750],\n",
       "          [ 0.4121, -0.6602,  0.0352,  ...,  0.9648, -0.8789, -1.1016]],\n",
       "\n",
       "         [[ 0.0074,  0.0918,  0.0391,  ...,  0.7539,  1.2578,  0.3203],\n",
       "          [ 5.0000,  4.8750,  1.8125,  ..., -2.2031,  0.8242, -2.5312],\n",
       "          [ 4.2500,  3.6094,  1.4922,  ..., -1.6172,  0.3477, -3.0312],\n",
       "          [ 0.2754,  0.4648,  0.9922,  ..., -2.0781, -0.6484, -1.7500]],\n",
       "\n",
       "         [[ 0.0237,  0.0200, -0.0200,  ...,  1.3281,  0.6992, -1.5938],\n",
       "          [-0.3086,  2.5312, -1.0938,  ..., -0.6523, -1.2734,  0.6875],\n",
       "          [-2.0000,  1.1094, -1.0938,  ..., -0.4551, -0.0298,  0.6328],\n",
       "          [ 0.3477, -0.3516, -1.2578,  ...,  0.3359,  1.4453, -0.3086]]]],\n",
       "       device='mps:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), tensor([[[[ 4.3640e-03,  5.5237e-03, -2.7100e-02,  ..., -2.3956e-03,\n",
       "           -5.0964e-03,  7.4707e-02],\n",
       "          [-2.1289e-01,  1.0986e-02, -4.3945e-02,  ..., -3.5156e-02,\n",
       "            3.4912e-02, -1.1377e-01],\n",
       "          [-9.4238e-02, -1.8188e-02, -5.8838e-02,  ..., -4.2725e-02,\n",
       "            1.8750e-01,  4.2725e-02],\n",
       "          [ 2.9785e-02,  2.0752e-02, -1.0132e-02,  ..., -3.1982e-02,\n",
       "            4.3701e-02, -3.4943e-03]],\n",
       "\n",
       "         [[-1.0529e-03,  9.9945e-04,  2.0790e-04,  ...,  9.6893e-04,\n",
       "            1.9836e-03, -6.5231e-04],\n",
       "          [ 8.7402e-02,  2.3730e-01, -3.9307e-02,  ..., -4.6631e-02,\n",
       "            1.7285e-01, -4.7852e-02],\n",
       "          [-8.2520e-02, -2.2949e-01,  5.8350e-02,  ...,  1.6211e-01,\n",
       "            4.1211e-01, -2.2583e-02],\n",
       "          [-5.1575e-03,  1.7944e-02, -2.2217e-02,  ..., -1.3733e-03,\n",
       "           -6.9580e-03, -1.2207e-02]],\n",
       "\n",
       "         [[-4.7607e-03, -5.2185e-03, -8.5449e-04,  ...,  4.9210e-04,\n",
       "           -2.4109e-03, -1.0223e-03],\n",
       "          [ 5.6396e-02, -7.2937e-03, -3.9551e-02,  ...,  3.5400e-02,\n",
       "           -1.6479e-02,  5.2490e-03],\n",
       "          [ 1.0352e-01, -1.3580e-03,  1.0132e-02,  ...,  3.5400e-02,\n",
       "           -4.5166e-02,  3.8574e-02],\n",
       "          [-4.4434e-02, -8.9722e-03, -2.0142e-02,  ..., -1.1658e-02,\n",
       "            8.4839e-03, -5.9570e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.9978e-03,  2.6855e-03, -6.0425e-03,  ..., -5.0659e-03,\n",
       "           -6.1951e-03,  1.8158e-03],\n",
       "          [-8.1543e-02, -1.8433e-02, -1.2158e-01,  ..., -6.8359e-02,\n",
       "            2.5879e-02,  1.0681e-02],\n",
       "          [-2.2583e-02,  6.8848e-02, -6.5918e-02,  ...,  3.9551e-02,\n",
       "           -5.6152e-03,  1.2451e-01],\n",
       "          [-8.8379e-02,  6.0303e-02,  5.4688e-02,  ...,  5.5542e-03,\n",
       "           -3.7842e-02, -2.4512e-01]],\n",
       "\n",
       "         [[ 6.4850e-04, -6.9046e-04,  2.6245e-03,  ..., -1.1253e-04,\n",
       "            2.9602e-03,  3.2043e-04],\n",
       "          [-3.9062e-02,  1.6504e-01,  4.2725e-03,  ..., -1.1670e-01,\n",
       "           -4.5166e-02,  1.5527e-01],\n",
       "          [ 1.0437e-02, -5.2979e-02, -8.2031e-02,  ...,  2.5757e-02,\n",
       "            3.3203e-02,  8.7891e-02],\n",
       "          [-4.6997e-03,  5.0659e-03, -2.1973e-02,  ...,  2.3560e-02,\n",
       "           -2.0020e-02,  5.1025e-02]],\n",
       "\n",
       "         [[ 5.4550e-04, -3.8452e-03,  1.3199e-03,  ...,  5.2795e-03,\n",
       "            3.8147e-05, -2.4414e-03],\n",
       "          [ 1.5039e-01,  7.9102e-02, -5.5176e-02,  ..., -1.1914e-01,\n",
       "            1.6602e-01,  2.0264e-02],\n",
       "          [ 4.2725e-02, -1.0938e-01, -1.9043e-01,  ..., -1.8066e-01,\n",
       "            4.1748e-02,  3.5645e-02],\n",
       "          [ 2.4780e-02, -1.3550e-02, -9.8267e-03,  ..., -4.0283e-03,\n",
       "            4.1504e-03, -4.4678e-02]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 4.1199e-03,  1.0223e-03,  6.3477e-03,  ..., -3.8086e-01,\n",
       "           -1.1523e-01,  1.0703e+00],\n",
       "          [ 2.3125e+00, -1.3516e+00, -1.1719e+00,  ..., -3.5938e-01,\n",
       "            5.8203e-01, -3.7656e+00],\n",
       "          [-1.8984e+00,  7.1094e-01, -2.4688e+00,  ..., -1.1953e+00,\n",
       "            1.8359e+00, -3.9062e+00],\n",
       "          [-6.3672e-01,  7.0312e-01, -3.3594e+00,  ...,  3.6133e-01,\n",
       "           -7.6172e-02, -3.3594e+00]],\n",
       "\n",
       "         [[ 1.8768e-03,  4.5166e-03,  1.5640e-03,  ...,  9.4141e-01,\n",
       "           -3.2617e-01, -1.0000e+00],\n",
       "          [ 1.3516e+00, -4.3750e+00, -1.1641e+00,  ..., -2.4219e+00,\n",
       "           -3.1836e-01,  2.4844e+00],\n",
       "          [-4.6875e+00, -4.4688e+00, -2.6094e+00,  ..., -1.0938e+00,\n",
       "            4.9805e-01,  3.5156e+00],\n",
       "          [-3.2031e+00, -2.7500e+00, -2.8750e+00,  ..., -2.3281e+00,\n",
       "           -1.7734e+00,  3.6875e+00]],\n",
       "\n",
       "         [[ 1.1047e-02,  3.1433e-03,  7.3547e-03,  ..., -3.2227e-01,\n",
       "           -2.0117e-01, -2.7148e-01],\n",
       "          [-1.6328e+00, -1.4141e+00, -1.5469e+00,  ..., -8.1250e-01,\n",
       "            2.2949e-01, -6.1719e-01],\n",
       "          [-4.2969e-01, -2.5312e+00, -1.9297e+00,  ..., -5.1172e-01,\n",
       "           -2.1973e-01,  2.4316e-01],\n",
       "          [ 9.2578e-01, -2.0312e+00, -1.3281e+00,  ..., -5.1562e-01,\n",
       "           -2.2852e-01, -6.7383e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.3782e-03,  4.4823e-04, -1.2589e-03,  ...,  6.6016e-01,\n",
       "            8.0859e-01,  5.3516e-01],\n",
       "          [ 1.4141e+00, -2.3438e-01,  2.4688e+00,  ..., -6.9922e-01,\n",
       "           -1.3984e+00, -5.2188e+00],\n",
       "          [-3.7188e+00,  2.0312e+00,  1.4844e+00,  ..., -1.9766e+00,\n",
       "           -3.1719e+00, -3.5156e+00],\n",
       "          [-5.0938e+00,  3.0625e+00,  6.0156e-01,  ..., -2.5625e+00,\n",
       "           -5.7500e+00, -2.0000e+00]],\n",
       "\n",
       "         [[-6.8665e-03,  7.6675e-04, -2.7618e-03,  ...,  4.1504e-03,\n",
       "            2.4707e-01, -7.1094e-01],\n",
       "          [ 3.6250e+00, -1.8672e+00,  3.2500e+00,  ..., -8.1641e-01,\n",
       "           -4.5898e-01,  2.6250e+00],\n",
       "          [-2.1250e+00, -4.2500e+00,  1.8594e+00,  ...,  5.1172e-01,\n",
       "           -1.9219e+00,  2.8906e+00],\n",
       "          [-6.1562e+00, -3.2500e+00, -1.9922e-01,  ...,  2.8750e+00,\n",
       "           -9.6680e-02,  3.3438e+00]],\n",
       "\n",
       "         [[-9.6130e-04,  1.1730e-04, -2.4109e-03,  ..., -2.5195e-01,\n",
       "            1.5234e-01, -1.0156e+00],\n",
       "          [ 1.3203e+00,  1.9062e+00,  1.2188e+00,  ...,  3.5352e-01,\n",
       "           -5.5908e-02,  3.3203e-01],\n",
       "          [-1.5703e+00,  1.7188e+00, -2.1875e-01,  ...,  2.3145e-01,\n",
       "            1.0625e+00,  1.5781e+00],\n",
       "          [-1.4297e+00,  1.2344e+00,  2.4121e-01,  ...,  4.0039e-01,\n",
       "            2.2188e+00,  1.3047e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 3.1433e-03, -5.0735e-04, -4.7112e-04,  ..., -1.0192e-05,\n",
       "            1.8539e-03,  2.3956e-03],\n",
       "          [ 6.4453e-02, -3.7891e-01,  4.7874e-04,  ...,  6.6406e-02,\n",
       "            1.7090e-01, -5.6641e-02],\n",
       "          [ 2.1680e-01,  7.9346e-03, -1.9434e-01,  ...,  4.4922e-01,\n",
       "           -3.3203e-02, -1.0547e-01],\n",
       "          [ 1.2158e-01, -3.1641e-01, -7.6172e-02,  ..., -1.2158e-01,\n",
       "           -1.3574e-01, -1.6699e-01]],\n",
       "\n",
       "         [[ 2.9144e-03, -1.5182e-03, -3.2043e-04,  ..., -2.5749e-04,\n",
       "           -2.3556e-04,  1.9646e-04],\n",
       "          [-2.3730e-01, -1.6724e-02,  6.6895e-02,  ..., -1.4355e-01,\n",
       "           -1.4258e-01, -1.0693e-01],\n",
       "          [-1.7676e-01,  1.9434e-01,  1.0693e-01,  ..., -2.3730e-01,\n",
       "            6.7871e-02, -2.5586e-01],\n",
       "          [-2.0508e-01,  9.3750e-02,  2.7148e-01,  ..., -1.2109e-01,\n",
       "            9.5703e-02, -6.7383e-02]],\n",
       "\n",
       "         [[-1.8921e-03, -5.0735e-04,  2.5024e-03,  ...,  3.0365e-03,\n",
       "           -2.8992e-03, -1.2891e-01],\n",
       "          [-1.3672e-01,  3.6621e-02,  8.6914e-02,  ...,  2.9492e-01,\n",
       "           -1.2451e-02,  4.3164e-01],\n",
       "          [-8.1055e-02,  8.5938e-02,  1.8164e-01,  ..., -9.1553e-03,\n",
       "            2.2949e-01,  5.4688e-01],\n",
       "          [ 5.4932e-03, -3.5553e-03,  2.4414e-02,  ..., -2.3633e-01,\n",
       "            3.9795e-02,  4.5117e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.5994e-04,  2.8992e-03, -2.1729e-02,  ...,  1.1841e-02,\n",
       "            1.5335e-03, -7.8201e-04],\n",
       "          [-1.0986e-01,  1.3046e-03, -4.4434e-02,  ...,  2.2852e-01,\n",
       "           -1.5820e-01, -1.0156e-01],\n",
       "          [-5.2246e-02,  1.5332e-01,  7.7148e-02,  ...,  2.0117e-01,\n",
       "            6.0547e-02, -1.0400e-01],\n",
       "          [ 7.2266e-02, -2.8711e-01,  1.1670e-01,  ...,  1.2988e-01,\n",
       "            8.9722e-03, -8.3496e-02]],\n",
       "\n",
       "         [[-1.6327e-03,  1.5411e-03, -2.0142e-03,  ..., -1.0300e-03,\n",
       "           -3.1662e-04,  1.7776e-03],\n",
       "          [-8.1543e-02, -1.1182e-01, -1.7383e-01,  ..., -5.2734e-02,\n",
       "           -1.7166e-03,  3.0078e-01],\n",
       "          [-5.3223e-02,  2.7539e-01, -1.5039e-01,  ...,  1.2695e-01,\n",
       "           -2.7148e-01,  5.3223e-02],\n",
       "          [ 7.0312e-02, -5.3955e-02,  9.0332e-02,  ..., -7.4219e-02,\n",
       "           -1.5430e-01,  1.9531e-01]],\n",
       "\n",
       "         [[-3.5706e-03, -9.0332e-03, -3.8757e-03,  ..., -2.1362e-02,\n",
       "            6.4697e-03,  3.5706e-03],\n",
       "          [-2.4316e-01,  1.8555e-02, -7.0496e-03,  ...,  8.7891e-02,\n",
       "           -2.5781e-01,  2.1387e-01],\n",
       "          [ 5.0293e-02, -4.7607e-02, -2.0264e-02,  ..., -1.5259e-02,\n",
       "            2.0605e-01,  2.9907e-03],\n",
       "          [-9.3750e-02,  9.3262e-02, -3.5889e-02,  ..., -5.7861e-02,\n",
       "            2.8320e-01,  8.5449e-02]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.9368e-03,  1.5106e-03,  4.0054e-04,  ...,  1.3379e-01,\n",
       "           -1.8125e+00, -6.7188e-01],\n",
       "          [ 1.0156e-01,  3.2656e+00, -3.5156e-02,  ...,  6.6406e-01,\n",
       "            4.2812e+00,  8.5547e-01],\n",
       "          [ 2.4219e+00,  1.6328e+00, -2.1875e+00,  ...,  3.4180e-01,\n",
       "            4.8438e+00,  1.3906e+00],\n",
       "          [ 1.5000e+00,  6.6016e-01, -2.7500e+00,  ...,  5.3516e-01,\n",
       "            4.7812e+00,  1.0000e+00]],\n",
       "\n",
       "         [[-4.7302e-03,  1.9684e-03, -3.1128e-03,  ...,  4.6680e-01,\n",
       "           -3.5938e-01, -2.3145e-01],\n",
       "          [ 8.3594e-01,  3.6914e-01, -5.5078e-01,  ...,  2.3828e-01,\n",
       "            1.9141e+00, -1.6016e+00],\n",
       "          [ 2.0312e+00,  2.1094e+00, -1.2969e+00,  ..., -3.5547e-01,\n",
       "            2.6172e-01,  2.5586e-01],\n",
       "          [ 5.2344e-01,  1.4453e+00, -1.5156e+00,  ..., -8.5156e-01,\n",
       "            5.5078e-01, -2.3926e-02]],\n",
       "\n",
       "         [[ 3.1586e-03,  2.7161e-03, -1.8463e-03,  ...,  2.3926e-01,\n",
       "           -1.5938e+00, -1.3672e+00],\n",
       "          [-1.0781e+00, -3.2812e+00, -1.3672e+00,  ...,  2.8711e-01,\n",
       "            3.1562e+00,  4.6562e+00],\n",
       "          [-5.2812e+00, -3.0000e+00, -4.5312e-01,  ...,  8.5156e-01,\n",
       "            4.3125e+00,  3.9375e+00],\n",
       "          [-2.0781e+00, -7.5391e-01, -1.7031e+00,  ...,  1.5312e+00,\n",
       "            2.1250e+00,  4.6875e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3275e-03,  4.6997e-03,  9.9182e-04,  ..., -7.2754e-02,\n",
       "           -4.1211e-01,  1.1484e+00],\n",
       "          [-3.2344e+00, -1.2109e+00,  2.3125e+00,  ...,  5.5078e-01,\n",
       "           -1.4141e+00, -2.4688e+00],\n",
       "          [ 1.5312e+00, -1.7383e-01,  2.8438e+00,  ...,  2.3438e+00,\n",
       "           -5.8594e-01, -2.4219e+00],\n",
       "          [ 2.8906e+00,  7.5391e-01,  2.0781e+00,  ...,  2.6406e+00,\n",
       "           -9.0820e-02, -3.4062e+00]],\n",
       "\n",
       "         [[-1.1520e-03, -2.5787e-03, -2.3346e-03,  ..., -5.5078e-01,\n",
       "            1.7578e-01, -7.8125e-02],\n",
       "          [-6.3672e-01, -1.7422e+00, -1.2891e+00,  ...,  5.3125e-01,\n",
       "            2.3047e-01, -8.8281e-01],\n",
       "          [ 1.1250e+00, -6.1328e-01, -1.2812e+00,  ...,  2.4062e+00,\n",
       "            7.5391e-01, -6.0938e-01],\n",
       "          [-3.6719e-01,  6.6797e-01, -3.3398e-01,  ...,  2.5000e+00,\n",
       "           -1.0938e+00, -2.6611e-02]],\n",
       "\n",
       "         [[-7.3547e-03, -1.8539e-03,  6.1035e-03,  ...,  2.3438e-01,\n",
       "           -1.8262e-01,  1.9629e-01],\n",
       "          [ 2.7031e+00,  2.2969e+00,  2.4844e+00,  ..., -2.1094e+00,\n",
       "            1.6504e-01, -7.3047e-01],\n",
       "          [ 3.2188e+00,  4.1562e+00,  3.9531e+00,  ..., -1.5859e+00,\n",
       "           -2.0215e-01, -1.3828e+00],\n",
       "          [-2.2461e-01,  2.5625e+00,  2.6406e+00,  ...,  3.0469e-01,\n",
       "            6.1328e-01,  8.7402e-02]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 1.5488e-03, -9.4986e-04,  3.2196e-03,  ...,  2.0905e-03,\n",
       "            1.5450e-04, -1.5030e-03],\n",
       "          [-3.7695e-01,  3.3398e-01,  3.3594e-01,  ...,  1.6602e-01,\n",
       "           -7.7637e-02,  5.1514e-02],\n",
       "          [ 5.6250e-01,  9.5215e-02, -3.1836e-01,  ...,  5.5237e-03,\n",
       "            3.7109e-01,  4.3359e-01],\n",
       "          [ 7.0312e-01,  3.2617e-01, -2.2559e-01,  ..., -4.8584e-02,\n",
       "            2.5586e-01,  6.9922e-01]],\n",
       "\n",
       "         [[-2.2095e-02,  1.0400e-01,  6.4468e-04,  ...,  3.7384e-03,\n",
       "           -1.3657e-03, -2.5482e-03],\n",
       "          [ 1.3574e-01,  2.2266e-01, -1.1169e-02,  ..., -1.9043e-01,\n",
       "            9.5703e-02, -2.4414e-01],\n",
       "          [-4.2236e-02,  4.6094e-01, -1.1523e-01,  ...,  4.9219e-01,\n",
       "           -1.7773e-01, -5.1025e-02],\n",
       "          [ 2.4902e-01,  4.6631e-02, -2.4780e-02,  ...,  5.3223e-02,\n",
       "            2.5787e-03, -1.0254e-01]],\n",
       "\n",
       "         [[-4.8256e-04, -1.6308e-04, -8.1253e-04,  ...,  6.6757e-04,\n",
       "           -1.5717e-03, -3.8452e-03],\n",
       "          [-1.2061e-01, -2.1387e-01, -2.4707e-01,  ..., -2.0508e-01,\n",
       "            2.1582e-01, -8.6719e-01],\n",
       "          [-5.1562e-01,  2.7734e-01, -3.6328e-01,  ...,  5.2979e-02,\n",
       "           -1.0156e-01, -1.8750e-01],\n",
       "          [-3.7109e-02, -2.4872e-03, -3.7109e-01,  ..., -8.2031e-02,\n",
       "           -9.6680e-02, -6.2012e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.3722e-03,  7.3624e-04, -6.3324e-04,  ..., -2.2168e-01,\n",
       "           -1.7319e-03, -1.7548e-04],\n",
       "          [-1.3965e-01, -2.2559e-01,  9.3750e-02,  ...,  5.9375e-01,\n",
       "            5.8203e-01, -1.8848e-01],\n",
       "          [-3.9453e-01, -5.9766e-01, -1.6895e-01,  ...,  8.3984e-01,\n",
       "           -1.6309e-01,  1.0620e-02],\n",
       "          [-1.8652e-01, -2.1680e-01,  3.2031e-01,  ...,  8.5156e-01,\n",
       "           -1.4160e-01, -6.7871e-02]],\n",
       "\n",
       "         [[ 3.9673e-03,  3.4180e-03, -1.0986e-03,  ..., -8.5831e-04,\n",
       "            1.9550e-04, -7.0095e-05],\n",
       "          [ 1.2793e-01, -1.9165e-02,  2.0117e-01,  ..., -4.7119e-02,\n",
       "            4.6143e-02, -3.2471e-02],\n",
       "          [ 1.7578e-01,  6.9824e-02, -9.9121e-02,  ...,  1.7578e-02,\n",
       "            8.4473e-02, -1.4160e-02],\n",
       "          [-2.7148e-01,  1.8066e-01,  1.5137e-01,  ..., -1.5039e-01,\n",
       "           -2.5586e-01, -3.4570e-01]],\n",
       "\n",
       "         [[-5.0049e-03,  1.3367e-02, -1.7166e-03,  ...,  7.3624e-04,\n",
       "           -1.7738e-04, -1.1444e-03],\n",
       "          [-3.4912e-02, -1.1182e-01,  2.1875e-01,  ...,  3.2501e-03,\n",
       "           -1.8066e-01, -3.6523e-01],\n",
       "          [ 9.1309e-02,  1.0547e-01,  2.5781e-01,  ...,  1.3672e-01,\n",
       "           -3.3594e-01,  2.6562e-01],\n",
       "          [ 3.7891e-01,  3.6377e-02,  2.1973e-01,  ..., -4.1008e-04,\n",
       "            5.7373e-02,  1.3672e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.6479e-03,  6.7139e-03,  5.5542e-03,  ..., -1.0469e+00,\n",
       "            1.1641e+00,  9.4922e-01],\n",
       "          [ 1.5938e+00, -3.5938e-01, -1.0312e+00,  ...,  4.8047e-01,\n",
       "            1.2422e+00, -2.0703e-01],\n",
       "          [ 2.0312e+00,  1.4609e+00, -1.8438e+00,  ...,  4.9805e-01,\n",
       "            2.7344e+00, -3.9258e-01],\n",
       "          [ 6.6406e-01,  1.0625e+00, -2.5156e+00,  ..., -3.9844e-01,\n",
       "            2.6406e+00,  4.3555e-01]],\n",
       "\n",
       "         [[ 2.1820e-03, -2.7161e-03, -3.0060e-03,  ..., -1.3965e-01,\n",
       "            6.2891e-01,  1.7578e-01],\n",
       "          [ 3.1875e+00,  1.9609e+00,  3.0625e+00,  ...,  1.3086e-01,\n",
       "           -6.6016e-01, -5.3516e-01],\n",
       "          [-9.3750e-02,  1.7969e-01,  2.3594e+00,  ...,  1.4844e+00,\n",
       "            9.6094e-01,  6.4062e-01],\n",
       "          [-1.0234e+00, -7.2656e-01,  2.9375e+00,  ..., -1.2422e+00,\n",
       "           -1.3770e-01,  3.1641e-01]],\n",
       "\n",
       "         [[ 1.9989e-03,  1.7776e-03,  6.9809e-04,  ..., -1.8848e-01,\n",
       "            2.0469e+00, -1.8984e+00],\n",
       "          [ 2.1406e+00,  1.5938e+00, -1.4922e+00,  ...,  6.7383e-02,\n",
       "           -4.7188e+00,  5.1875e+00],\n",
       "          [ 4.0625e+00,  3.5156e+00, -3.2812e+00,  ...,  2.5195e-01,\n",
       "           -5.0938e+00,  5.9375e+00],\n",
       "          [ 9.3750e-02,  3.2188e+00, -3.3125e+00,  ..., -7.7344e-01,\n",
       "           -5.0625e+00,  6.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.2360e-03, -2.0752e-03,  2.5635e-03,  ..., -4.9805e-01,\n",
       "           -1.2578e+00,  2.6367e-01],\n",
       "          [ 3.7109e-01, -6.9922e-01,  1.1797e+00,  ..., -1.3984e+00,\n",
       "           -4.7607e-02,  1.1875e+00],\n",
       "          [-1.8203e+00, -6.0547e-02,  1.2422e+00,  ..., -1.5859e+00,\n",
       "           -1.6504e-01,  6.4062e-01],\n",
       "          [-1.6484e+00,  1.9629e-01,  1.2188e+00,  ..., -8.3984e-01,\n",
       "            2.2656e-01,  9.2969e-01]],\n",
       "\n",
       "         [[ 3.5706e-03, -2.7771e-03, -6.6223e-03,  ...,  1.3203e+00,\n",
       "           -1.1797e+00,  1.8750e+00],\n",
       "          [-1.1875e+00, -1.3359e+00, -1.4297e+00,  ..., -1.2344e+00,\n",
       "           -6.7188e-01, -3.2344e+00],\n",
       "          [ 1.8203e+00,  2.2656e-01, -3.6719e-01,  ...,  1.1016e+00,\n",
       "           -4.1602e-01, -3.6719e+00],\n",
       "          [ 2.7188e+00,  2.7539e-01, -7.8125e-03,  ..., -3.5352e-01,\n",
       "            4.1016e-01, -3.6250e+00]],\n",
       "\n",
       "         [[-6.0425e-03,  7.2937e-03,  6.4392e-03,  ..., -6.4062e-01,\n",
       "           -2.5156e+00,  1.1953e+00],\n",
       "          [ 5.6250e-01,  2.1406e+00,  2.3906e+00,  ..., -7.0703e-01,\n",
       "            5.6562e+00,  2.5195e-01],\n",
       "          [-2.3750e+00,  1.8125e+00,  2.3750e+00,  ..., -2.8125e+00,\n",
       "            5.9375e+00, -7.6172e-01],\n",
       "          [-8.0469e-01,  1.7734e+00,  2.8594e+00,  ..., -1.8047e+00,\n",
       "            6.8438e+00, -1.1875e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-7.7438e-04, -5.2490e-03, -3.9101e-04,  ..., -2.7466e-03,\n",
       "            1.8692e-04, -1.4114e-03],\n",
       "          [-2.0410e-01,  7.7344e-01,  7.2937e-03,  ..., -5.4688e-02,\n",
       "            2.6367e-01,  4.7461e-01],\n",
       "          [-1.6699e-01,  2.5586e-01,  8.5449e-03,  ..., -3.9648e-01,\n",
       "           -3.6133e-01,  1.1816e-01],\n",
       "          [-5.4297e-01,  2.6758e-01, -4.9414e-01,  ...,  3.1982e-02,\n",
       "            3.0273e-01,  4.7461e-01]],\n",
       "\n",
       "         [[ 2.4719e-03,  9.9182e-04, -1.0529e-03,  ...,  3.7956e-04,\n",
       "           -6.1417e-04, -3.7551e-06],\n",
       "          [-4.9316e-02, -3.3984e-01, -1.1182e-01,  ...,  1.6968e-02,\n",
       "           -3.3984e-01,  1.1133e-01],\n",
       "          [ 1.0864e-02,  1.2793e-01,  4.6875e-01,  ..., -4.6143e-02,\n",
       "           -8.3984e-02,  2.1582e-01],\n",
       "          [ 1.6504e-01, -2.0312e-01, -1.6797e-01,  ..., -1.6699e-01,\n",
       "            1.3184e-01,  6.6406e-01]],\n",
       "\n",
       "         [[ 2.7924e-03, -3.5553e-03, -2.3041e-03,  ...,  1.0757e-03,\n",
       "           -5.5542e-03, -2.4414e-02],\n",
       "          [ 2.6953e-01,  4.1992e-02,  2.5195e-01,  ..., -2.1973e-02,\n",
       "            1.4941e-01,  5.0781e-01],\n",
       "          [ 2.8125e-01,  3.2959e-02, -1.3574e-01,  ..., -2.8198e-02,\n",
       "            2.0312e-01,  1.3199e-03],\n",
       "          [-1.1377e-01,  4.6289e-01, -2.0703e-01,  ...,  1.5430e-01,\n",
       "            5.4321e-03,  3.1250e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.6861e-03,  9.6512e-04, -5.0354e-04,  ...,  1.0147e-03,\n",
       "           -2.3041e-03,  2.0294e-03],\n",
       "          [-5.4297e-01, -3.4961e-01, -2.3535e-01,  ...,  2.8442e-02,\n",
       "           -3.0859e-01, -1.7578e-01],\n",
       "          [ 7.1289e-02, -3.6523e-01, -2.4023e-01,  ...,  2.0605e-01,\n",
       "           -2.6562e-01, -2.6758e-01],\n",
       "          [-5.9326e-02, -4.0430e-01,  2.2754e-01,  ..., -1.9141e-01,\n",
       "           -3.0859e-01,  4.5703e-01]],\n",
       "\n",
       "         [[-7.5531e-04,  2.9602e-03, -1.5106e-03,  ..., -2.5558e-04,\n",
       "           -1.3504e-03, -4.6730e-04],\n",
       "          [-3.2227e-01,  1.8262e-01, -2.4902e-01,  ...,  4.0820e-01,\n",
       "            1.8945e-01, -1.3086e-01],\n",
       "          [-2.9297e-01,  2.6953e-01, -6.3281e-01,  ...,  4.1211e-01,\n",
       "           -1.0986e-01,  1.8555e-01],\n",
       "          [-4.0430e-01,  2.7344e-01, -4.4922e-01,  ...,  8.5938e-02,\n",
       "            3.5938e-01, -4.1016e-01]],\n",
       "\n",
       "         [[-2.8229e-03, -3.8452e-03, -7.0953e-04,  ..., -8.3542e-04,\n",
       "            2.4033e-04, -2.4872e-03],\n",
       "          [ 6.3281e-01,  3.1250e-01, -1.0498e-01,  ...,  1.4465e-02,\n",
       "            1.5182e-03,  6.0059e-02],\n",
       "          [ 2.5977e-01, -1.2109e-01,  4.0527e-02,  ..., -1.0303e-01,\n",
       "            2.1851e-02,  1.0498e-02],\n",
       "          [ 1.6504e-01,  3.2471e-02, -2.8906e-01,  ...,  5.2344e-01,\n",
       "           -2.9492e-01, -1.6211e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-7.0801e-03,  9.7656e-04,  9.7656e-03,  ..., -2.5195e-01,\n",
       "            1.1875e+00, -4.4727e-01],\n",
       "          [-1.4219e+00, -1.2422e+00,  9.3750e-01,  ..., -6.0156e-01,\n",
       "            5.0537e-02, -2.7031e+00],\n",
       "          [ 1.3906e+00, -8.1641e-01, -8.1250e-01,  ..., -5.1562e-01,\n",
       "            2.8906e+00, -4.0039e-01],\n",
       "          [ 1.2734e+00, -1.1797e+00, -1.1016e+00,  ..., -5.1758e-02,\n",
       "            1.7266e+00, -2.3438e+00]],\n",
       "\n",
       "         [[-9.4223e-04,  2.8381e-03,  1.1673e-03,  ..., -9.5703e-01,\n",
       "           -1.0234e+00,  7.4609e-01],\n",
       "          [-1.5234e+00,  2.4844e+00, -1.7500e+00,  ..., -4.0312e+00,\n",
       "           -2.0156e+00,  3.2422e-01],\n",
       "          [ 3.0625e+00, -4.4531e-01, -3.7109e-01,  ..., -2.9531e+00,\n",
       "           -1.4688e+00,  4.7852e-01],\n",
       "          [ 1.5469e+00, -2.0469e+00, -6.8359e-02,  ..., -3.8750e+00,\n",
       "           -1.0156e+00,  9.0820e-02]],\n",
       "\n",
       "         [[ 3.6926e-03, -5.1575e-03,  3.8910e-04,  ...,  1.1094e+00,\n",
       "           -1.7656e+00,  8.1641e-01],\n",
       "          [ 1.1562e+00, -2.5781e-01, -3.0312e+00,  ...,  1.2354e-01,\n",
       "            2.4062e+00,  9.5215e-02],\n",
       "          [-2.0938e+00,  8.4766e-01, -2.6250e+00,  ..., -3.8574e-02,\n",
       "            3.8438e+00,  6.2988e-02],\n",
       "          [-4.1562e+00,  2.0312e+00, -2.2031e+00,  ...,  5.5078e-01,\n",
       "            3.8438e+00,  7.4609e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.5449e-03, -8.1177e-03, -1.0925e-02,  ..., -2.0117e-01,\n",
       "            4.1406e-01,  1.5469e+00],\n",
       "          [ 1.3438e+00, -1.0312e+00,  1.2734e+00,  ..., -1.8906e+00,\n",
       "           -4.0039e-01, -1.0469e+00],\n",
       "          [ 3.2812e-01,  2.0312e-01,  1.5625e+00,  ..., -1.2344e+00,\n",
       "            6.6016e-01, -2.0156e+00],\n",
       "          [-1.3281e+00,  2.1875e+00,  2.2656e+00,  ..., -8.1250e-01,\n",
       "            4.7656e-01, -1.8047e+00]],\n",
       "\n",
       "         [[ 3.0975e-03, -3.3722e-03, -3.8452e-03,  ...,  3.0664e-01,\n",
       "            4.7363e-02, -2.7148e-01],\n",
       "          [ 2.0000e+00,  1.9062e+00,  9.7656e-01,  ..., -2.5586e-01,\n",
       "            8.0859e-01,  3.8086e-01],\n",
       "          [-1.8203e+00, -1.6992e-01,  2.1250e+00,  ..., -1.3672e+00,\n",
       "           -1.4219e+00, -1.3125e+00],\n",
       "          [-3.0312e+00, -1.9531e+00,  2.4844e+00,  ...,  8.8672e-01,\n",
       "           -1.6953e+00, -3.9453e-01]],\n",
       "\n",
       "         [[-9.8267e-03,  2.6398e-03, -3.3112e-03,  ..., -8.0078e-01,\n",
       "            1.0781e+00,  2.1191e-01],\n",
       "          [-6.4062e-01,  1.1406e+00, -9.0234e-01,  ..., -1.2031e+00,\n",
       "            1.9844e+00,  2.2656e+00],\n",
       "          [ 2.8906e+00,  4.4336e-01, -1.9375e+00,  ..., -3.2344e+00,\n",
       "           -6.5625e-01,  1.6875e+00],\n",
       "          [ 2.0312e+00, -1.6113e-01, -2.7500e+00,  ..., -2.0156e+00,\n",
       "            2.5391e-01,  1.0078e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 2.1973e-03, -2.2949e-01,  2.1362e-04,  ..., -1.0376e-03,\n",
       "            5.5847e-03,  1.6785e-03],\n",
       "          [-3.4961e-01,  5.1953e-01, -1.3672e-01,  ..., -2.1094e-01,\n",
       "           -2.6758e-01, -5.7812e-01],\n",
       "          [-2.3804e-02,  7.7734e-01,  1.4941e-01,  ...,  6.0156e-01,\n",
       "            1.8188e-02, -2.0695e-04],\n",
       "          [-2.8125e-01,  7.1484e-01, -5.0000e-01,  ...,  6.4453e-01,\n",
       "            9.4238e-02, -2.3535e-01]],\n",
       "\n",
       "         [[-1.3580e-03, -3.9368e-03, -4.6387e-03,  ..., -1.2665e-03,\n",
       "            3.0975e-03, -9.7656e-04],\n",
       "          [ 1.4941e-01,  3.4180e-01,  1.7188e-01,  ..., -1.6016e-01,\n",
       "            2.5586e-01,  1.9727e-01],\n",
       "          [ 6.3477e-02,  3.7695e-01,  5.8350e-02,  ...,  2.2754e-01,\n",
       "            3.1641e-01,  1.7480e-01],\n",
       "          [ 1.8311e-02, -7.9102e-02,  1.8555e-01,  ...,  6.7871e-02,\n",
       "            4.2578e-01,  2.6172e-01]],\n",
       "\n",
       "         [[-1.4709e-02,  7.9727e-04,  1.0010e-02,  ...,  1.5106e-03,\n",
       "            1.1230e-02,  4.5776e-03],\n",
       "          [ 2.4902e-01,  7.8125e-02,  6.6895e-02,  ..., -7.2754e-02,\n",
       "            2.3828e-01, -2.5977e-01],\n",
       "          [ 4.6631e-02, -9.9121e-02, -1.2354e-01,  ..., -3.1641e-01,\n",
       "           -1.3672e-01,  1.6699e-01],\n",
       "          [ 3.0151e-02,  1.6504e-01, -1.8359e-01,  ..., -2.0142e-02,\n",
       "            1.8359e-01, -2.9492e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8158e-03,  2.5787e-03, -1.3580e-03,  ..., -2.7313e-03,\n",
       "           -1.6251e-03, -3.5095e-03],\n",
       "          [ 4.1211e-01,  4.4678e-02, -1.5820e-01,  ..., -6.1279e-02,\n",
       "           -4.6631e-02,  7.3730e-02],\n",
       "          [ 1.2305e-01,  1.2988e-01, -3.2422e-01,  ..., -1.5625e-01,\n",
       "           -1.2512e-02,  1.7383e-01],\n",
       "          [ 2.4902e-02, -8.4229e-03, -2.4023e-01,  ...,  1.1108e-02,\n",
       "            2.4414e-01,  7.7515e-03]],\n",
       "\n",
       "         [[ 2.2030e-04, -5.0659e-03,  3.7384e-04,  ...,  4.0283e-02,\n",
       "            1.3199e-03, -1.5640e-03],\n",
       "          [-2.0386e-02,  5.7422e-01, -2.7148e-01,  ..., -3.2227e-02,\n",
       "            1.7700e-02, -3.0884e-02],\n",
       "          [-2.7930e-01,  4.3750e-01, -1.7188e-01,  ...,  1.4746e-01,\n",
       "           -5.8105e-02,  1.7334e-02],\n",
       "          [-6.9336e-02,  7.3828e-01, -2.9102e-01,  ...,  9.1309e-02,\n",
       "           -1.0840e-01, -2.0020e-01]],\n",
       "\n",
       "         [[-6.7444e-03,  3.7079e-03,  2.1362e-03,  ...,  4.5166e-03,\n",
       "            9.3579e-06, -1.4954e-03],\n",
       "          [-9.3750e-02, -2.2095e-02, -6.8359e-02,  ...,  1.8555e-01,\n",
       "           -1.0596e-01, -4.4189e-02],\n",
       "          [ 9.9609e-02,  1.3611e-02, -2.4609e-01,  ..., -2.2559e-01,\n",
       "           -2.3047e-01, -9.8145e-02],\n",
       "          [-1.1621e-01,  7.8125e-02,  1.2158e-01,  ..., -7.6904e-03,\n",
       "            9.1797e-02, -1.4355e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.4027e-03,  1.6251e-03,  7.2479e-05,  ..., -9.0625e-01,\n",
       "           -1.2656e+00,  1.8438e+00],\n",
       "          [ 1.2812e+00,  3.2031e+00,  2.2656e+00,  ...,  2.6367e-01,\n",
       "           -2.8906e-01, -3.9375e+00],\n",
       "          [ 3.1562e+00,  1.9375e+00,  3.0156e+00,  ...,  9.2969e-01,\n",
       "           -3.0859e-01, -4.2188e+00],\n",
       "          [ 2.1875e+00,  9.6875e-01,  2.3438e+00,  ...,  5.6250e-01,\n",
       "           -6.3672e-01, -4.9688e+00]],\n",
       "\n",
       "         [[-1.3962e-03,  7.6294e-04, -2.1820e-03,  ..., -1.6250e+00,\n",
       "           -1.6875e+00,  5.9375e-01],\n",
       "          [-9.9219e-01,  1.0469e+00, -1.5391e+00,  ...,  3.0938e+00,\n",
       "            4.4062e+00,  1.5078e+00],\n",
       "          [-3.7695e-01,  2.8906e-01, -1.0625e+00,  ...,  2.8906e+00,\n",
       "            5.8125e+00,  4.5898e-01],\n",
       "          [ 8.2031e-01, -6.3281e-01, -7.1094e-01,  ...,  1.7109e+00,\n",
       "            6.2500e+00,  6.2500e-01]],\n",
       "\n",
       "         [[ 5.4321e-03,  8.9722e-03,  7.6294e-03,  ..., -5.6250e-01,\n",
       "            3.9062e-01, -6.7969e-01],\n",
       "          [-1.5723e-01,  1.8750e-01,  5.6250e-01,  ...,  3.7695e-01,\n",
       "           -8.5938e-01,  1.0156e+00],\n",
       "          [ 1.3574e-01, -2.3535e-01, -2.8516e-01,  ...,  9.6484e-01,\n",
       "           -2.6562e-01,  2.7969e+00],\n",
       "          [-4.8828e-04,  1.3086e-01, -3.5352e-01,  ...,  1.9688e+00,\n",
       "           -1.0645e-01,  5.5625e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.3853e-03, -3.8910e-03,  4.4556e-03,  ...,  1.9062e+00,\n",
       "           -1.0781e+00, -1.6406e-01],\n",
       "          [ 6.2891e-01,  1.8359e-01,  1.1172e+00,  ..., -2.1562e+00,\n",
       "           -5.3125e-01,  1.0469e+00],\n",
       "          [ 3.0859e-01,  2.0781e+00,  1.0156e+00,  ..., -2.8438e+00,\n",
       "            4.5703e-01, -1.1328e+00],\n",
       "          [-2.5469e+00,  2.6562e+00, -2.6953e-01,  ..., -2.7500e+00,\n",
       "            1.8125e+00, -7.5391e-01]],\n",
       "\n",
       "         [[ 4.3945e-03, -5.2490e-03,  2.8381e-03,  ...,  9.1797e-01,\n",
       "            2.0781e+00,  1.5156e+00],\n",
       "          [-4.8047e-01, -1.1641e+00, -9.2969e-01,  ...,  1.7500e+00,\n",
       "           -2.6094e+00, -4.5898e-01],\n",
       "          [ 8.4375e-01, -6.3672e-01, -1.2969e+00,  ...,  1.6562e+00,\n",
       "           -3.2812e+00, -1.3359e+00],\n",
       "          [ 2.0625e+00,  4.6875e-02, -1.0391e+00,  ...,  1.5625e+00,\n",
       "           -3.5000e+00, -1.6875e+00]],\n",
       "\n",
       "         [[ 1.0071e-02, -3.9673e-03,  2.6398e-03,  ..., -1.1641e+00,\n",
       "            2.3633e-01, -8.9062e-01],\n",
       "          [-2.3906e+00,  3.3438e+00,  2.3281e+00,  ..., -6.9531e-01,\n",
       "            1.6250e+00, -1.7891e+00],\n",
       "          [-1.8438e+00,  1.9375e+00,  6.3281e-01,  ..., -1.5234e+00,\n",
       "            1.1572e-01, -1.9141e+00],\n",
       "          [-4.2188e-01,  7.8516e-01,  2.2461e-01,  ..., -1.1016e+00,\n",
       "           -2.0410e-01, -1.8828e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 6.7749e-03,  1.0864e-02,  2.5024e-03,  ...,  5.8594e-03,\n",
       "           -4.7363e-02,  2.1362e-02],\n",
       "          [-7.5195e-02, -1.6895e-01, -1.6211e-01,  ..., -3.5742e-01,\n",
       "            3.1836e-01, -1.5723e-01],\n",
       "          [ 1.5820e-01, -9.4238e-02,  2.0215e-01,  ...,  1.0693e-01,\n",
       "            3.4180e-01, -2.1387e-01],\n",
       "          [ 4.2578e-01,  2.7930e-01,  3.7695e-01,  ...,  5.5176e-02,\n",
       "            2.0020e-01, -1.8262e-01]],\n",
       "\n",
       "         [[ 7.5912e-04, -2.9449e-03,  2.1118e-02,  ...,  5.5237e-03,\n",
       "           -4.3297e-04,  4.1199e-03],\n",
       "          [ 4.1504e-02,  9.1797e-02, -1.6699e-01,  ...,  1.6504e-01,\n",
       "           -2.9297e-01, -2.5781e-01],\n",
       "          [-1.0107e-01,  1.8750e-01, -3.7695e-01,  ..., -3.4766e-01,\n",
       "           -1.5527e-01,  6.5430e-02],\n",
       "          [-1.4551e-01,  1.4551e-01, -3.9453e-01,  ..., -4.9414e-01,\n",
       "           -5.1562e-01,  1.3477e-01]],\n",
       "\n",
       "         [[ 2.1851e-02, -1.0757e-03,  7.8201e-04,  ..., -4.0817e-04,\n",
       "           -4.7913e-03, -5.1880e-04],\n",
       "          [-6.1328e-01,  2.8516e-01, -3.8281e-01,  ..., -1.4746e-01,\n",
       "            2.7930e-01, -5.8350e-02],\n",
       "          [-5.0781e-01, -8.3496e-02, -6.2891e-01,  ..., -2.2461e-01,\n",
       "            3.6328e-01,  1.3281e-01],\n",
       "          [-5.2344e-01, -1.5332e-01, -5.4297e-01,  ..., -2.9688e-01,\n",
       "            3.2422e-01,  1.8945e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.4250e-03,  7.7820e-03,  9.0942e-03,  ..., -1.0315e-02,\n",
       "            8.7280e-03, -3.6774e-03],\n",
       "          [ 5.6396e-02,  1.7871e-01,  3.5400e-02,  ...,  2.2461e-01,\n",
       "            7.4158e-03,  2.0996e-02],\n",
       "          [ 1.3477e-01, -2.7344e-01, -3.6316e-03,  ..., -1.1426e-01,\n",
       "            1.2061e-01, -1.7383e-01],\n",
       "          [ 1.1572e-01, -8.6914e-02, -3.3447e-02,  ..., -1.1865e-01,\n",
       "            2.0117e-01, -7.3730e-02]],\n",
       "\n",
       "         [[ 1.0132e-02,  1.8799e-02, -5.2185e-03,  ..., -1.2158e-01,\n",
       "           -1.4893e-02,  7.4463e-03],\n",
       "          [ 3.5156e-02,  1.7456e-02,  2.9297e-01,  ...,  1.5234e-01,\n",
       "            2.8125e-01, -2.2852e-01],\n",
       "          [ 3.4375e-01,  1.3965e-01,  1.5527e-01,  ..., -4.1748e-02,\n",
       "            6.9336e-02,  3.3789e-01],\n",
       "          [ 1.3965e-01, -3.0469e-01,  3.0078e-01,  ...,  1.8750e-01,\n",
       "            3.1250e-01,  4.8828e-02]],\n",
       "\n",
       "         [[-7.1411e-03,  6.9580e-03,  1.1169e-02,  ..., -2.6703e-03,\n",
       "            6.4392e-03, -1.5945e-03],\n",
       "          [-1.3379e-01, -1.5332e-01,  2.0410e-01,  ..., -1.1328e-01,\n",
       "            3.1128e-03, -5.2490e-02],\n",
       "          [-3.2031e-01, -3.7695e-01, -7.4707e-02,  ..., -1.1523e-01,\n",
       "           -1.0938e-01,  2.9297e-01],\n",
       "          [-3.6133e-01, -2.6562e-01,  4.8828e-01,  ...,  2.6245e-02,\n",
       "           -2.3438e-02,  1.9238e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.8757e-03, -5.2185e-03,  8.0872e-04,  ..., -1.0234e+00,\n",
       "            8.9062e-01,  8.5547e-01],\n",
       "          [ 3.8438e+00,  2.9062e+00, -2.5312e+00,  ..., -7.2266e-02,\n",
       "           -8.6914e-02,  6.2891e-01],\n",
       "          [ 5.5625e+00,  3.5156e-01, -5.1875e+00,  ...,  7.7344e-01,\n",
       "           -3.3984e-01,  8.9844e-01],\n",
       "          [ 1.8672e+00, -3.2969e+00, -5.2500e+00,  ...,  1.4531e+00,\n",
       "           -8.4375e-01,  3.1055e-01]],\n",
       "\n",
       "         [[-7.3853e-03,  5.7373e-03,  2.1667e-03,  ..., -4.1992e-01,\n",
       "            1.1328e+00,  2.3730e-01],\n",
       "          [ 1.9219e+00, -1.4453e+00, -7.1094e-01,  ..., -7.8516e-01,\n",
       "           -3.7109e-02,  1.0156e+00],\n",
       "          [-1.2578e+00, -2.3125e+00, -3.9844e-01,  ..., -8.7891e-01,\n",
       "           -3.7500e-01,  1.8750e+00],\n",
       "          [-3.6406e+00, -2.0469e+00, -7.8906e-01,  ..., -6.5234e-01,\n",
       "            9.1797e-02,  1.3359e+00]],\n",
       "\n",
       "         [[-6.4087e-03,  1.6708e-03, -2.2583e-03,  ..., -1.4453e+00,\n",
       "           -9.6875e-01,  1.3594e+00],\n",
       "          [-8.3008e-02,  3.1641e-01,  8.2031e-01,  ...,  5.9688e+00,\n",
       "            4.5000e+00, -8.3125e+00],\n",
       "          [ 1.7285e-01,  3.1128e-02,  1.7383e-01,  ...,  4.0625e+00,\n",
       "            1.1406e+00, -7.9375e+00],\n",
       "          [ 5.7422e-01, -1.8848e-01,  1.7285e-01,  ...,  3.6719e+00,\n",
       "            6.6406e-01, -4.6875e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.1635e-04, -1.7776e-03,  1.7853e-03,  ...,  1.0078e+00,\n",
       "           -4.9023e-01, -2.5625e+00],\n",
       "          [-8.0859e-01,  9.7656e-01,  1.1641e+00,  ...,  2.5000e-01,\n",
       "           -1.3281e+00,  2.5781e+00],\n",
       "          [-9.6484e-01,  2.8906e-01,  2.7930e-01,  ...,  7.3047e-01,\n",
       "           -1.0938e+00,  3.9375e+00],\n",
       "          [-1.5918e-01, -9.4141e-01,  7.9688e-01,  ..., -1.9238e-01,\n",
       "           -1.1328e+00,  4.5312e+00]],\n",
       "\n",
       "         [[ 3.0060e-03, -3.0823e-03,  5.9128e-04,  ..., -9.1016e-01,\n",
       "           -2.1719e+00, -1.3281e+00],\n",
       "          [-1.1797e+00,  1.1172e+00, -1.0000e+00,  ...,  3.2812e+00,\n",
       "            1.4375e+00, -1.6641e+00],\n",
       "          [-1.4766e+00,  2.1719e+00, -1.2969e+00,  ...,  2.9688e+00,\n",
       "            1.6172e+00,  2.6758e-01],\n",
       "          [-1.1641e+00,  1.7031e+00, -2.5625e+00,  ...,  1.7891e+00,\n",
       "            3.2031e+00,  2.3535e-01]],\n",
       "\n",
       "         [[ 5.1880e-03,  4.3335e-03, -7.8964e-04,  ...,  8.5547e-01,\n",
       "            2.7031e+00, -3.2969e+00],\n",
       "          [-4.1406e-01, -4.6875e-01,  4.8633e-01,  ...,  3.3594e+00,\n",
       "            1.8750e-01,  2.9375e+00],\n",
       "          [ 1.3672e+00,  1.9531e-02,  1.2891e+00,  ...,  3.2969e+00,\n",
       "            1.9434e-01,  5.0938e+00],\n",
       "          [ 1.1484e+00,  2.1875e-01,  2.7969e+00,  ...,  1.8281e+00,\n",
       "           -1.7773e-01,  5.1562e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 9.5215e-03,  2.8687e-02, -3.9673e-03,  ...,  6.9275e-03,\n",
       "            1.8921e-03, -1.9531e-02],\n",
       "          [ 3.0469e-01, -4.3750e-01, -1.0840e-01,  ...,  3.1641e-01,\n",
       "           -1.7676e-01,  1.3867e-01],\n",
       "          [-3.0884e-02,  3.3008e-01, -4.3945e-02,  ...,  1.7090e-01,\n",
       "           -2.3145e-01,  2.0312e-01],\n",
       "          [-7.6660e-02,  1.5137e-02, -2.2656e-01,  ...,  2.2705e-02,\n",
       "           -1.8262e-01,  2.4414e-01]],\n",
       "\n",
       "         [[ 5.8105e-02, -3.7079e-03, -2.6855e-03,  ..., -2.1515e-03,\n",
       "            1.4893e-02, -9.3994e-03],\n",
       "          [ 2.4902e-01,  2.8906e-01,  8.9844e-02,  ..., -2.9492e-01,\n",
       "            8.2520e-02,  1.6309e-01],\n",
       "          [ 5.3467e-02, -7.8125e-02,  8.8379e-02,  ...,  4.9805e-01,\n",
       "           -4.0430e-01, -8.5449e-02],\n",
       "          [ 5.2734e-01,  2.6367e-01,  7.9590e-02,  ..., -1.4453e-01,\n",
       "           -6.2500e-01, -1.6895e-01]],\n",
       "\n",
       "         [[ 3.6049e-04, -5.5237e-03,  1.1749e-03,  ..., -5.4016e-03,\n",
       "            1.4099e-02, -1.5182e-03],\n",
       "          [ 2.0996e-01,  8.1055e-02,  7.3853e-03,  ...,  2.3242e-01,\n",
       "           -6.2988e-02,  3.6377e-02],\n",
       "          [ 2.2168e-01,  3.6914e-01, -2.3346e-03,  ...,  3.4180e-01,\n",
       "            7.8125e-02,  4.1992e-02],\n",
       "          [ 8.6914e-02,  2.9492e-01, -1.7969e-01,  ..., -4.6631e-02,\n",
       "           -6.9885e-03, -2.0801e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.4861e-03,  4.7302e-03,  3.6774e-03,  ..., -6.5002e-03,\n",
       "            1.7944e-02, -1.2512e-02],\n",
       "          [ 1.9141e-01,  2.9492e-01, -3.4961e-01,  ...,  3.0273e-01,\n",
       "           -1.3672e-01, -1.1670e-01],\n",
       "          [ 2.1973e-01,  1.1182e-01, -2.5391e-01,  ...,  3.1250e-02,\n",
       "           -1.4746e-01, -3.2227e-01],\n",
       "          [ 3.0859e-01,  1.0254e-01, -8.1543e-02,  ...,  1.7480e-01,\n",
       "           -1.5625e-01, -1.9897e-02]],\n",
       "\n",
       "         [[ 1.1353e-02,  1.0910e-03,  7.8735e-03,  ..., -1.9897e-02,\n",
       "           -3.4637e-03,  7.9346e-03],\n",
       "          [-1.5430e-01,  7.5195e-02,  2.7734e-01,  ...,  2.3145e-01,\n",
       "            1.8750e-01,  2.1606e-02],\n",
       "          [-2.0898e-01, -9.8633e-02,  4.7266e-01,  ...,  2.6172e-01,\n",
       "           -1.6113e-01, -5.7617e-02],\n",
       "          [ 3.0273e-01,  5.8105e-02,  2.7539e-01,  ...,  7.8906e-01,\n",
       "            4.7266e-01,  3.5938e-01]],\n",
       "\n",
       "         [[ 1.9043e-02, -2.8076e-03, -5.2490e-02,  ...,  3.5889e-02,\n",
       "           -6.5002e-03,  4.5586e-04],\n",
       "          [-1.5820e-01,  4.2725e-02,  6.1279e-02,  ...,  2.1191e-01,\n",
       "            2.1582e-01, -6.9336e-02],\n",
       "          [ 2.6978e-02,  1.0223e-03,  9.2773e-02,  ...,  2.8906e-01,\n",
       "            1.0400e-01, -3.1494e-02],\n",
       "          [ 1.8555e-01, -5.7129e-02,  4.4678e-02,  ...,  1.8555e-01,\n",
       "           -6.7383e-02,  8.9355e-02]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.8997e-03, -7.5684e-03, -5.1880e-03,  ...,  6.3672e-01,\n",
       "           -2.3340e-01, -1.2402e-01],\n",
       "          [-1.1250e+00,  2.9102e-01,  8.8281e-01,  ..., -2.2949e-01,\n",
       "            2.9102e-01,  2.4062e+00],\n",
       "          [-1.3359e+00,  1.4297e+00,  1.0234e+00,  ...,  5.7422e-01,\n",
       "            1.1484e+00,  2.0938e+00],\n",
       "          [-4.1016e-01,  3.0312e+00,  3.2422e-01,  ...,  2.7188e+00,\n",
       "           -1.5820e-01,  1.4219e+00]],\n",
       "\n",
       "         [[ 2.1515e-03,  4.3030e-03, -7.0496e-03,  ...,  9.0625e-01,\n",
       "            1.7969e-01, -4.1797e-01],\n",
       "          [-1.2656e+00,  1.7812e+00,  3.5156e-02,  ..., -1.9844e+00,\n",
       "           -1.6797e+00, -1.2734e+00],\n",
       "          [-7.5000e-01, -6.4062e-01,  8.3594e-01,  ..., -2.3594e+00,\n",
       "           -1.3594e+00, -8.9062e-01],\n",
       "          [ 7.3828e-01, -2.2656e+00,  2.7031e+00,  ..., -2.8750e+00,\n",
       "           -8.7500e-01,  1.1523e-01]],\n",
       "\n",
       "         [[ 1.4038e-03,  1.8234e-03, -1.3809e-03,  ...,  3.2617e-01,\n",
       "            6.7188e-01, -8.6914e-02],\n",
       "          [-1.7578e-01, -2.4805e-01, -3.7109e-01,  ...,  1.9141e+00,\n",
       "           -1.2500e+00, -9.0625e-01],\n",
       "          [-1.4375e+00, -2.4170e-02, -1.0469e+00,  ...,  2.0312e+00,\n",
       "            4.2725e-02, -1.8281e+00],\n",
       "          [-1.0547e+00,  4.8047e-01, -3.2031e+00,  ...,  1.3984e+00,\n",
       "            9.6094e-01, -1.2578e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.4087e-03,  5.6076e-04,  7.3547e-03,  ..., -4.3359e-01,\n",
       "            1.6875e+00, -8.8867e-02],\n",
       "          [ 9.0625e-01, -7.5000e-01,  1.4688e+00,  ...,  3.9453e-01,\n",
       "           -2.7500e+00, -2.5156e+00],\n",
       "          [-4.3359e-01,  6.7188e-01, -2.8125e-01,  ...,  4.9219e-01,\n",
       "           -2.4375e+00, -2.7656e+00],\n",
       "          [-1.2500e+00,  1.7344e+00, -7.6562e-01,  ...,  6.7578e-01,\n",
       "           -1.9062e+00, -2.8906e-01]],\n",
       "\n",
       "         [[ 6.7749e-03, -6.2256e-03, -9.2773e-03,  ..., -3.7354e-02,\n",
       "           -1.4766e+00, -7.9297e-01],\n",
       "          [-2.3633e-01,  4.1406e-01,  4.7852e-01,  ...,  1.2188e+00,\n",
       "           -1.7656e+00,  1.3516e+00],\n",
       "          [ 5.7031e-01,  5.4688e-02,  3.5742e-01,  ...,  3.8086e-01,\n",
       "           -1.9238e-01,  1.3672e+00],\n",
       "          [ 1.1484e+00, -1.9531e-01,  2.1094e-01,  ..., -3.7500e-01,\n",
       "           -1.0059e-01,  1.4531e+00]],\n",
       "\n",
       "         [[-1.9150e-03, -2.9445e-05, -5.3101e-03,  ...,  4.8828e-01,\n",
       "            2.0625e+00,  1.8359e+00],\n",
       "          [-1.8750e-01, -1.1094e+00, -5.7812e-01,  ..., -3.2031e-01,\n",
       "            3.9258e-01, -9.8438e-01],\n",
       "          [-1.5781e+00,  1.9531e-01, -8.4766e-01,  ..., -3.3984e-01,\n",
       "            1.0078e+00, -2.7812e+00],\n",
       "          [-2.1406e+00,  1.4844e+00, -1.0859e+00,  ..., -8.9453e-01,\n",
       "           -1.2891e+00, -3.1094e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-0.0040, -0.0442,  0.0176,  ...,  0.0267, -0.0034, -0.0330],\n",
       "          [ 0.1592,  0.2871, -0.2080,  ..., -0.4375, -0.0410,  0.1973],\n",
       "          [ 0.3652,  0.0640, -0.1670,  ..., -0.0032,  0.1172,  0.2402],\n",
       "          [-0.1475, -0.1475,  0.0396,  ..., -0.5820,  0.1465,  0.2100]],\n",
       "\n",
       "         [[ 0.0042,  0.0064, -0.0054,  ..., -0.0038, -0.0035, -0.0013],\n",
       "          [-0.0267,  0.2178,  0.4297,  ..., -0.1631,  0.1680, -0.2715],\n",
       "          [ 0.0121,  0.3613,  0.4238,  ...,  0.1455,  0.0272, -0.5156],\n",
       "          [-0.1426,  0.1611,  0.6016,  ...,  0.1592, -0.1338, -0.6875]],\n",
       "\n",
       "         [[ 0.0087,  0.0063,  0.0120,  ..., -0.0055,  0.0212,  0.0030],\n",
       "          [-0.1172, -0.1963, -0.0405,  ...,  0.0232, -0.6680, -0.2119],\n",
       "          [ 0.1602, -0.5703, -0.0757,  ..., -0.4160, -0.4121,  0.3125],\n",
       "          [ 0.3398, -0.4844,  0.1152,  ..., -0.4160, -0.1611, -0.1230]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0023, -0.0033,  0.0023,  ..., -0.0116,  0.0019,  0.0108],\n",
       "          [-0.0405,  0.3516, -0.0064,  ...,  0.0239,  0.1709,  0.1719],\n",
       "          [-0.3125,  0.2100, -0.0209,  ..., -0.0576, -0.1699,  0.0491],\n",
       "          [-0.1235,  0.0410, -0.4824,  ..., -0.0654, -0.4414, -0.1689]],\n",
       "\n",
       "         [[ 0.0376,  0.0079,  0.0045,  ...,  0.0047, -0.0032, -0.0116],\n",
       "          [-0.0552, -0.0510, -0.1045,  ..., -0.0845, -0.0933,  0.0265],\n",
       "          [ 0.1206, -0.1777, -0.1445,  ...,  0.2383, -0.1514, -0.2197],\n",
       "          [-0.0096,  0.1650, -0.3164,  ...,  0.2207, -0.3086,  0.1069]],\n",
       "\n",
       "         [[ 0.0146,  0.0076, -0.0114,  ...,  0.0063, -0.0009,  0.0077],\n",
       "          [-0.3066, -0.0693, -0.1553,  ...,  0.0913, -0.2500, -0.1406],\n",
       "          [-0.0430, -0.1836, -0.0349,  ..., -0.1289, -0.3633, -0.3984],\n",
       "          [-0.2715, -0.2656, -0.4043,  ..., -0.1167, -0.0063, -0.5352]]]],\n",
       "       device='mps:0', dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)), (tensor([[[[-9.0332e-03,  7.8735e-03, -3.8452e-03,  ...,  3.6133e-01,\n",
       "           -7.2656e-01,  3.2227e-01],\n",
       "          [-2.6758e-01,  3.3594e-01, -8.2422e-01,  ..., -1.3594e+00,\n",
       "            2.6953e-01,  1.1406e+00],\n",
       "          [-1.8457e-01,  5.6641e-01, -3.3984e-01,  ..., -1.1016e+00,\n",
       "            2.0938e+00, -3.4570e-01],\n",
       "          [-7.5781e-01,  1.5547e+00,  3.1641e-01,  ...,  8.6914e-02,\n",
       "            2.9688e+00,  3.1250e-01]],\n",
       "\n",
       "         [[ 3.4180e-03, -7.6904e-03, -4.1504e-03,  ..., -6.1719e-01,\n",
       "           -5.5469e-01,  6.4453e-01],\n",
       "          [-7.1094e-01,  4.9219e-01, -5.5664e-02,  ...,  8.1055e-02,\n",
       "            3.6719e-01, -1.4844e+00],\n",
       "          [-5.6250e-01, -5.0391e-01, -1.0781e+00,  ..., -1.8359e+00,\n",
       "           -8.8281e-01, -2.5312e+00],\n",
       "          [ 1.0312e+00, -1.8750e+00, -2.2969e+00,  ..., -3.2344e+00,\n",
       "           -1.9609e+00, -1.2578e+00]],\n",
       "\n",
       "         [[-4.9744e-03, -3.4332e-05,  2.9144e-03,  ..., -1.0449e-01,\n",
       "            1.1250e+00, -1.6113e-01],\n",
       "          [-1.3438e+00, -8.2422e-01,  1.6562e+00,  ..., -5.2344e-01,\n",
       "            4.1406e-01,  1.4688e+00],\n",
       "          [ 9.8047e-01,  1.3125e+00,  5.5859e-01,  ...,  3.8672e-01,\n",
       "            3.0469e-01,  1.4453e+00],\n",
       "          [ 2.8438e+00,  3.6562e+00, -5.4688e-02,  ...,  4.2383e-01,\n",
       "           -3.2422e-01,  9.8828e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.6621e-04, -4.4250e-04, -3.7537e-03,  ...,  1.7031e+00,\n",
       "            1.7266e+00,  3.4766e-01],\n",
       "          [-1.0742e-01,  3.3203e-01, -4.6875e-02,  ..., -1.3359e+00,\n",
       "           -9.9219e-01, -4.5625e+00],\n",
       "          [-9.9219e-01,  7.4219e-01,  4.1406e-01,  ..., -1.9766e+00,\n",
       "           -2.0625e+00, -4.6250e+00],\n",
       "          [-1.0625e+00,  1.3125e+00,  1.1484e+00,  ..., -3.3125e+00,\n",
       "           -1.0938e+00, -3.8906e+00]],\n",
       "\n",
       "         [[ 5.6152e-03, -1.6861e-03, -3.9673e-03,  ..., -9.8438e-01,\n",
       "            4.2578e-01,  6.4062e-01],\n",
       "          [ 5.9375e-01,  1.1875e+00,  2.7500e+00,  ..., -1.7969e+00,\n",
       "            1.0938e+00,  6.8359e-02],\n",
       "          [-4.8750e+00, -1.6719e+00,  1.9922e+00,  ..., -1.6406e+00,\n",
       "            1.4844e+00,  9.8047e-01],\n",
       "          [-6.1875e+00, -2.6094e+00,  4.1602e-01,  ..., -1.6406e+00,\n",
       "            1.3516e+00,  6.1328e-01]],\n",
       "\n",
       "         [[ 5.7678e-03, -4.3106e-04, -9.8877e-03,  ...,  6.6016e-01,\n",
       "            1.1641e+00,  8.3984e-01],\n",
       "          [ 2.2363e-01,  6.3672e-01, -1.1484e+00,  ...,  5.7422e-01,\n",
       "           -3.5938e+00,  2.0469e+00],\n",
       "          [-1.0234e+00,  7.9590e-02, -7.2656e-01,  ...,  2.7500e+00,\n",
       "           -3.1719e+00,  1.8359e-01],\n",
       "          [-1.1875e+00, -9.9609e-01, -3.7695e-01,  ...,  3.9062e+00,\n",
       "           -1.7734e+00,  5.9766e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 5.1575e-03, -7.1716e-03,  3.2959e-03,  ...,  3.6430e-04,\n",
       "            1.0864e-02,  4.3457e-02],\n",
       "          [ 4.0820e-01,  1.0107e-01, -2.5146e-02,  ...,  3.3691e-02,\n",
       "            1.9287e-02, -1.4160e-01],\n",
       "          [ 6.8359e-01, -2.0508e-01,  8.3008e-02,  ..., -2.9102e-01,\n",
       "            4.8828e-02, -4.6094e-01],\n",
       "          [ 1.4648e-01,  1.8848e-01,  2.9297e-01,  ..., -2.2363e-01,\n",
       "           -8.2520e-02,  2.1484e-01]],\n",
       "\n",
       "         [[ 6.2561e-03,  2.6093e-03,  2.3193e-03,  ..., -2.6245e-02,\n",
       "            6.9336e-02, -9.0942e-03],\n",
       "          [-5.1172e-01,  1.9043e-02,  1.2451e-01,  ...,  3.2227e-02,\n",
       "            4.9561e-02,  2.4414e-02],\n",
       "          [-4.0234e-01,  2.7344e-01,  3.8672e-01,  ...,  1.7188e-01,\n",
       "           -1.7578e-01, -3.4180e-02],\n",
       "          [-1.9824e-01, -3.3447e-02, -2.3438e-02,  ...,  2.7344e-01,\n",
       "            5.3223e-02, -2.6611e-02]],\n",
       "\n",
       "         [[-1.1292e-02,  1.4343e-03,  2.1973e-02,  ...,  1.9775e-02,\n",
       "            4.5471e-03, -1.6309e-01],\n",
       "          [-2.7148e-01, -1.7676e-01,  3.7305e-01,  ..., -2.4023e-01,\n",
       "            4.4922e-01,  1.6016e-01],\n",
       "          [-1.1670e-01, -3.1250e-01,  2.4609e-01,  ...,  8.3984e-02,\n",
       "            4.2578e-01,  2.9492e-01],\n",
       "          [-3.5938e-01, -6.9336e-02,  1.3245e-02,  ...,  1.3867e-01,\n",
       "            3.6914e-01,  2.8320e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1738e-03,  4.9219e-01, -1.7212e-02,  ..., -4.3945e-03,\n",
       "            1.5503e-02, -2.8687e-02],\n",
       "          [ 7.9102e-02, -3.1641e-01, -2.4023e-01,  ...,  2.9492e-01,\n",
       "           -4.2969e-02,  9.2773e-02],\n",
       "          [-2.0508e-01, -1.2305e-01, -9.2773e-02,  ...,  1.0107e-01,\n",
       "           -4.5898e-01,  1.4099e-02],\n",
       "          [-2.2754e-01,  3.3203e-01,  3.0078e-01,  ..., -4.1797e-01,\n",
       "           -4.6680e-01,  5.3516e-01]],\n",
       "\n",
       "         [[-3.0975e-03,  8.4839e-03, -7.5073e-03,  ..., -1.1139e-03,\n",
       "            4.1199e-03,  2.4512e-01],\n",
       "          [ 3.1836e-01, -4.2969e-02,  2.1387e-01,  ..., -1.6895e-01,\n",
       "           -1.2573e-02, -1.5527e-01],\n",
       "          [-3.0664e-01,  1.2012e-01,  3.6133e-01,  ..., -1.0938e-01,\n",
       "           -2.3535e-01, -9.5215e-03],\n",
       "          [-5.5908e-02,  1.8066e-01,  2.0410e-01,  ..., -4.1504e-02,\n",
       "           -8.3984e-02, -8.9722e-03]],\n",
       "\n",
       "         [[-2.6550e-03,  2.7924e-03, -5.2490e-03,  ..., -1.0693e-01,\n",
       "            8.2397e-03,  8.7402e-02],\n",
       "          [ 2.1680e-01,  3.6133e-01,  2.1582e-01,  ...,  1.0391e+00,\n",
       "           -8.8501e-03, -4.3164e-01],\n",
       "          [ 2.3438e-01,  2.0215e-01,  5.1562e-01,  ...,  7.4609e-01,\n",
       "           -4.3213e-02, -4.8242e-01],\n",
       "          [ 4.7070e-01,  2.2461e-02,  3.7109e-01,  ...,  1.4160e-01,\n",
       "            1.8750e-01,  1.4551e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-8.0566e-03, -4.4250e-03,  2.1362e-03,  ...,  7.5000e-01,\n",
       "            7.1484e-01, -6.0156e-01],\n",
       "          [-5.4297e-01, -6.5625e-01,  1.2422e+00,  ...,  2.7031e+00,\n",
       "           -3.9258e-01,  5.0781e-01],\n",
       "          [ 5.8984e-01, -2.3926e-01,  5.8984e-01,  ...,  4.0312e+00,\n",
       "            3.5742e-01, -8.5156e-01],\n",
       "          [ 1.2344e+00,  1.7969e+00,  9.1016e-01,  ...,  2.7969e+00,\n",
       "            4.0625e-01, -1.0859e+00]],\n",
       "\n",
       "         [[ 5.1880e-03,  1.4709e-02, -2.0123e-04,  ..., -1.2656e+00,\n",
       "           -1.5625e+00,  7.4609e-01],\n",
       "          [-2.3594e+00,  8.7891e-01, -9.0234e-01,  ..., -1.3750e+00,\n",
       "           -1.9688e+00,  1.3906e+00],\n",
       "          [-2.9375e+00,  1.7773e-01, -8.1250e-01,  ..., -6.9141e-01,\n",
       "           -2.2031e+00,  8.7500e-01],\n",
       "          [-3.5938e-01, -2.9062e+00, -1.9219e+00,  ..., -1.0156e+00,\n",
       "           -1.0547e+00,  1.5391e+00]],\n",
       "\n",
       "         [[ 9.0332e-03,  1.8463e-03,  5.0659e-03,  ...,  5.3906e-01,\n",
       "            4.8438e-01, -1.1953e+00],\n",
       "          [ 8.5547e-01,  1.0859e+00,  2.1875e-01,  ..., -1.5156e+00,\n",
       "           -1.4766e+00,  1.2812e+00],\n",
       "          [ 7.5781e-01, -5.4688e-02,  9.7656e-01,  ..., -1.1250e+00,\n",
       "           -1.0312e+00,  9.4141e-01],\n",
       "          [-1.4531e+00, -1.8984e+00,  1.6875e+00,  ..., -2.1719e+00,\n",
       "           -1.0156e+00,  1.1094e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.8300e-03,  3.0212e-03,  6.9275e-03,  ..., -1.1953e+00,\n",
       "            2.6406e+00, -7.8516e-01],\n",
       "          [ 3.1836e-01,  7.3438e-01,  7.8125e-01,  ..., -1.5938e+00,\n",
       "           -4.1875e+00, -1.4297e+00],\n",
       "          [ 1.2578e+00,  4.9805e-02,  5.3516e-01,  ..., -2.5625e+00,\n",
       "           -4.9688e+00, -1.4688e+00],\n",
       "          [-2.1680e-01, -6.0938e-01,  2.1387e-01,  ..., -2.2344e+00,\n",
       "           -5.4062e+00, -2.4844e+00]],\n",
       "\n",
       "         [[-1.7242e-03,  4.4556e-03, -7.4768e-04,  ..., -5.4932e-02,\n",
       "           -1.8594e+00, -5.0000e-01],\n",
       "          [-4.1406e-01, -1.7656e+00,  4.7852e-02,  ...,  1.9297e+00,\n",
       "            3.1719e+00,  1.6328e+00],\n",
       "          [ 1.7578e+00, -2.3125e+00,  1.0391e+00,  ...,  1.5859e+00,\n",
       "            3.8281e+00,  2.0156e+00],\n",
       "          [ 2.3906e+00, -1.3281e+00,  1.0703e+00,  ...,  2.6875e+00,\n",
       "            4.0312e+00,  1.4062e+00]],\n",
       "\n",
       "         [[ 1.9226e-03,  7.5378e-03,  1.2970e-03,  ...,  7.2656e-01,\n",
       "            6.6016e-01,  4.9414e-01],\n",
       "          [ 3.0156e+00,  1.8281e+00,  3.0938e+00,  ..., -9.1797e-01,\n",
       "            2.8594e+00,  9.5703e-01],\n",
       "          [ 4.0625e+00,  3.0938e+00,  2.0312e+00,  ..., -7.6562e-01,\n",
       "            1.3594e+00,  1.2031e+00],\n",
       "          [ 1.2891e-01,  2.1250e+00,  1.3281e-01,  ..., -1.1250e+00,\n",
       "            2.1250e+00,  8.7500e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-0.0171, -0.0024,  0.0069,  ...,  0.0058, -0.0166, -0.0017],\n",
       "          [-0.4238, -0.3457, -0.1309,  ..., -0.5000,  0.1514, -0.0903],\n",
       "          [-0.0500, -0.1084, -0.0840,  ..., -0.4297,  0.0630, -0.0182],\n",
       "          [-0.1416, -0.3418, -0.2676,  ..., -0.4785, -0.1553,  0.0378]],\n",
       "\n",
       "         [[-0.0182, -0.0146,  0.0260,  ..., -0.2168,  0.0156,  0.0037],\n",
       "          [-0.0249, -0.0315,  0.0447,  ..., -0.5391,  0.2891,  0.0325],\n",
       "          [-0.0820,  0.2334,  0.2480,  ..., -0.5430,  0.1611, -0.1543],\n",
       "          [-0.0776,  0.3516,  0.1777,  ..., -0.6953,  0.0417, -0.0640]],\n",
       "\n",
       "         [[ 0.0461, -0.0049,  0.0010,  ..., -0.0014, -0.0069,  0.0019],\n",
       "          [ 0.2236,  0.3926, -0.3613,  ..., -0.2109, -0.0996,  0.2178],\n",
       "          [ 0.7812, -0.1973, -0.2773,  ..., -0.3086, -0.1553, -0.0061],\n",
       "          [ 0.5742, -0.6289,  0.1924,  ..., -0.4023,  0.1172, -0.6055]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0116, -0.0074, -0.0035,  ...,  0.0071,  0.0050, -0.0045],\n",
       "          [-0.1963, -0.1045, -0.0693,  ..., -0.2637,  0.1172,  0.3965],\n",
       "          [-0.1396, -0.1011,  0.0649,  ..., -0.2441,  0.2676, -0.0337],\n",
       "          [-0.4922,  0.1504,  0.2754,  ...,  0.0347, -0.1797,  0.1572]],\n",
       "\n",
       "         [[-0.0041,  0.0364, -0.0147,  ...,  0.0698,  0.0056, -0.4121],\n",
       "          [-0.4180, -0.0840, -0.2422,  ...,  0.5312,  0.0571, -0.2051],\n",
       "          [-0.2217,  0.1270,  0.0270,  ...,  0.8672, -0.0476,  0.1924],\n",
       "          [-0.4238, -0.1270, -0.0898,  ...,  0.8867, -0.0654,  0.4121]],\n",
       "\n",
       "         [[-0.0178,  0.0045, -0.0095,  ...,  0.0085,  0.0011, -0.0020],\n",
       "          [-0.0527,  0.1426,  0.1133,  ..., -0.4316,  0.1357,  0.1416],\n",
       "          [-0.1436, -0.5312, -0.1631,  ..., -0.2637,  0.4434,  0.4473],\n",
       "          [-0.4922, -0.6445, -0.0280,  ...,  0.2168,  0.5000,  0.4180]]]],\n",
       "       device='mps:0', dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)), (tensor([[[[-8.9111e-03,  6.9809e-04,  3.0212e-03,  ...,  3.8281e-01,\n",
       "            1.0703e+00, -2.5391e-01],\n",
       "          [-2.5312e+00, -5.0391e-01,  2.7344e+00,  ...,  8.7500e-01,\n",
       "           -3.1719e+00, -1.4453e+00],\n",
       "          [ 1.2109e+00, -2.4062e+00,  2.3438e+00,  ..., -6.9922e-01,\n",
       "           -4.3438e+00, -1.2188e+00],\n",
       "          [ 3.2656e+00, -2.2188e+00,  2.2812e+00,  ...,  1.1484e+00,\n",
       "           -4.5625e+00, -1.1094e+00]],\n",
       "\n",
       "         [[-6.1417e-04, -1.2512e-03, -5.9204e-03,  ...,  5.4688e-01,\n",
       "            1.4160e-01, -2.4023e-01],\n",
       "          [-2.0156e+00, -1.3906e+00, -9.0625e-01,  ..., -1.2344e+00,\n",
       "           -5.7812e-01, -4.7461e-01],\n",
       "          [-1.5156e+00, -7.3438e-01, -3.9062e-03,  ..., -5.1172e-01,\n",
       "           -4.6484e-01, -9.3750e-02],\n",
       "          [-7.0312e-01,  6.1719e-01,  4.3359e-01,  ..., -8.6328e-01,\n",
       "            9.5703e-01,  1.1953e+00]],\n",
       "\n",
       "         [[ 3.7231e-03,  4.2114e-03, -2.0294e-03,  ..., -3.1836e-01,\n",
       "            2.2031e+00, -1.9336e-01],\n",
       "          [ 7.4219e-01,  1.1016e+00,  4.1797e-01,  ...,  3.3789e-01,\n",
       "           -4.3125e+00,  1.5547e+00],\n",
       "          [-2.9492e-01, -6.1719e-01,  1.1250e+00,  ...,  4.2969e-01,\n",
       "           -4.2812e+00,  1.1406e+00],\n",
       "          [-2.7188e+00, -1.7188e+00,  2.5625e+00,  ...,  2.7969e+00,\n",
       "           -3.6719e+00,  1.5234e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.8419e-04,  3.1433e-03, -1.3672e-02,  ..., -3.8719e-04,\n",
       "           -2.8320e-01,  2.7148e-01],\n",
       "          [-5.0000e-01,  1.2695e-01, -2.1484e-02,  ...,  1.1328e+00,\n",
       "            3.8281e+00, -7.8125e-01],\n",
       "          [-5.7422e-01,  2.0508e-01,  4.4922e-01,  ...,  6.5234e-01,\n",
       "            5.8125e+00, -1.7188e+00],\n",
       "          [-2.0703e-01,  2.7734e-01,  5.3125e-01,  ...,  2.2344e+00,\n",
       "            4.9375e+00, -3.5469e+00]],\n",
       "\n",
       "         [[-1.4801e-03, -6.5918e-03, -2.2888e-03,  ..., -9.1016e-01,\n",
       "            9.3750e-01, -1.0859e+00],\n",
       "          [-3.3203e-02,  2.5000e-01, -7.4219e-02,  ...,  5.1250e+00,\n",
       "           -1.3984e+00,  6.1328e-01],\n",
       "          [ 5.1758e-02, -2.6953e-01, -2.1875e-01,  ...,  3.7344e+00,\n",
       "           -1.6875e+00,  1.6562e+00],\n",
       "          [ 2.9297e-01, -1.8652e-01, -3.9844e-01,  ...,  5.0938e+00,\n",
       "           -1.6953e+00,  2.7812e+00]],\n",
       "\n",
       "         [[ 3.0975e-03, -2.6398e-03, -4.8523e-03,  ..., -8.9844e-02,\n",
       "            1.8672e+00,  5.7422e-01],\n",
       "          [ 1.4453e-01,  6.6016e-01, -1.8750e-01,  ...,  5.1270e-02,\n",
       "           -1.1797e+00,  7.3047e-01],\n",
       "          [ 1.6797e+00,  1.9609e+00, -1.4688e+00,  ...,  6.3281e-01,\n",
       "           -1.6406e+00,  9.2188e-01],\n",
       "          [ 1.1016e+00,  2.0938e+00, -3.2344e+00,  ...,  6.4062e-01,\n",
       "           -3.1562e+00,  1.5938e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 4.7913e-03,  2.7466e-03,  1.3046e-03,  ...,  2.6703e-03,\n",
       "           -7.2479e-04, -4.4250e-03],\n",
       "          [-7.7637e-02, -2.2949e-01, -4.3750e-01,  ..., -3.6523e-01,\n",
       "           -1.1377e-01, -3.2227e-01],\n",
       "          [ 1.7578e-01,  2.8125e-01, -5.6641e-01,  ..., -1.2988e-01,\n",
       "            8.8867e-02, -2.2363e-01],\n",
       "          [ 2.6562e-01, -1.2793e-01, -3.7695e-01,  ...,  1.0645e-01,\n",
       "            1.7773e-01,  2.3047e-01]],\n",
       "\n",
       "         [[ 2.6172e-01,  9.9487e-03, -1.0620e-02,  ..., -3.4790e-03,\n",
       "           -5.6458e-03, -8.4305e-04],\n",
       "          [-4.4922e-01, -1.7969e-01,  2.8442e-02,  ...,  3.9453e-01,\n",
       "            5.0293e-02, -1.2451e-01],\n",
       "          [ 2.9883e-01, -2.1240e-02, -3.2617e-01,  ...,  2.4902e-01,\n",
       "            2.3828e-01,  8.1055e-02],\n",
       "          [-1.9043e-01,  1.6992e-01,  4.3701e-02,  ...,  6.0156e-01,\n",
       "            3.3984e-01,  9.1934e-04]],\n",
       "\n",
       "         [[ 2.3315e-02,  1.7456e-02, -8.8215e-05,  ...,  1.3245e-02,\n",
       "           -3.8300e-03, -2.4707e-01],\n",
       "          [-4.0234e-01,  5.8203e-01, -2.6953e-01,  ..., -5.3516e-01,\n",
       "            1.5723e-01,  4.6094e-01],\n",
       "          [-3.6328e-01,  4.0820e-01, -2.6367e-01,  ..., -6.3672e-01,\n",
       "            4.9414e-01,  1.1621e-01],\n",
       "          [ 1.2695e-01, -3.7305e-01,  3.7842e-02,  ..., -2.1191e-01,\n",
       "            1.2158e-01,  4.2383e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1475e-02,  7.8735e-03, -1.6785e-03,  ...,  5.0049e-03,\n",
       "            1.3809e-03,  1.4801e-03],\n",
       "          [-9.0234e-01, -4.2236e-02,  1.7676e-01,  ...,  3.8818e-02,\n",
       "           -8.6060e-03, -6.0156e-01],\n",
       "          [-3.3398e-01, -2.2363e-01, -6.1523e-02,  ...,  3.6133e-01,\n",
       "           -2.9297e-01, -7.4219e-02],\n",
       "          [-1.8164e-01,  2.3804e-02, -6.2109e-01,  ..., -2.7148e-01,\n",
       "            1.8164e-01,  2.4219e-01]],\n",
       "\n",
       "         [[-3.4180e-03,  4.3640e-03,  9.2773e-03,  ...,  9.6512e-04,\n",
       "           -4.1580e-04,  2.4536e-02],\n",
       "          [ 1.9043e-01, -5.5859e-01,  3.4961e-01,  ...,  2.3145e-01,\n",
       "            1.7285e-01, -1.0791e-01],\n",
       "          [ 4.8242e-01, -1.0645e-01,  2.2949e-01,  ..., -1.7773e-01,\n",
       "            1.5332e-01,  3.8818e-02],\n",
       "          [ 2.7930e-01,  5.5908e-02, -5.5420e-02,  ...,  5.4688e-02,\n",
       "           -2.3560e-02, -2.0996e-01]],\n",
       "\n",
       "         [[ 8.3008e-03, -4.7913e-03,  5.4321e-03,  ...,  1.1536e-02,\n",
       "           -6.8970e-03, -7.8201e-04],\n",
       "          [ 3.7305e-01, -2.2168e-01, -2.4414e-03,  ...,  1.4453e-01,\n",
       "            1.1328e-01,  3.7537e-03],\n",
       "          [ 3.7598e-02,  1.5039e-01, -2.1484e-01,  ...,  1.6699e-01,\n",
       "           -1.4355e-01, -3.3008e-01],\n",
       "          [-3.6523e-01, -1.3281e-01,  9.1797e-02,  ...,  2.7148e-01,\n",
       "           -1.7578e-01,  1.4648e-02]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-5.9204e-03,  3.0365e-03, -2.1210e-03,  ...,  6.0547e-01,\n",
       "            1.4531e+00, -6.8750e-01],\n",
       "          [ 6.0938e-01, -6.6016e-01,  4.1992e-02,  ...,  9.1016e-01,\n",
       "           -2.9297e-01,  6.3281e-01],\n",
       "          [ 4.5703e-01, -2.9492e-01,  5.2344e-01,  ...,  4.2383e-01,\n",
       "           -5.4688e-01,  8.2031e-01],\n",
       "          [ 5.7129e-02,  7.9688e-01,  4.2188e-01,  ..., -3.6328e-01,\n",
       "           -5.8203e-01,  5.5859e-01]],\n",
       "\n",
       "         [[-7.4158e-03, -6.7444e-03,  4.9744e-03,  ...,  3.5547e-01,\n",
       "            2.5977e-01, -1.7734e+00],\n",
       "          [ 1.9336e-01, -6.6895e-02, -7.7148e-02,  ...,  3.3125e+00,\n",
       "           -4.1875e+00,  1.8203e+00],\n",
       "          [ 4.1016e-01,  1.6211e-01,  3.6328e-01,  ...,  3.3281e+00,\n",
       "           -6.5938e+00,  1.0469e+00],\n",
       "          [-2.4121e-01,  2.9492e-01, -4.5703e-01,  ...,  5.5625e+00,\n",
       "           -7.4688e+00,  2.0469e+00]],\n",
       "\n",
       "         [[ 1.4877e-03, -1.8845e-03,  5.6076e-04,  ...,  8.1250e-01,\n",
       "           -3.2617e-01, -4.6484e-01],\n",
       "          [ 5.6250e+00, -3.3594e+00,  3.2188e+00,  ...,  1.8047e+00,\n",
       "           -2.9492e-01, -9.9219e-01],\n",
       "          [ 5.7812e+00, -4.3438e+00,  2.2656e+00,  ...,  2.5156e+00,\n",
       "            5.3516e-01,  1.1475e-02],\n",
       "          [ 2.7344e-02, -2.8438e+00,  1.2109e+00,  ...,  2.9062e+00,\n",
       "           -5.8984e-01,  8.3594e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2085e-02,  1.2817e-03,  7.5073e-03,  ..., -3.0469e-01,\n",
       "            1.5820e-01,  2.4688e+00],\n",
       "          [ 2.0508e-02,  3.0273e-01, -9.0234e-01,  ..., -2.0312e+00,\n",
       "            1.5391e+00, -1.7031e+00],\n",
       "          [-7.8906e-01,  1.3281e+00,  9.3750e-01,  ..., -3.0156e+00,\n",
       "            2.3047e-01, -4.4688e+00],\n",
       "          [-3.4766e-01,  9.1406e-01,  7.9297e-01,  ..., -2.9375e+00,\n",
       "            1.1094e+00, -4.8438e+00]],\n",
       "\n",
       "         [[-6.6223e-03,  3.8300e-03,  2.6550e-03,  ...,  7.1875e-01,\n",
       "            4.9805e-01,  7.9688e-01],\n",
       "          [-2.7930e-01,  9.2969e-01, -9.5703e-01,  ...,  1.1172e+00,\n",
       "           -6.4844e-01,  1.4219e+00],\n",
       "          [ 1.2500e+00,  3.6328e-01, -1.4375e+00,  ..., -3.1836e-01,\n",
       "            1.1133e-01,  8.2812e-01],\n",
       "          [ 2.4688e+00, -5.6641e-01, -3.5938e-01,  ...,  6.0156e-01,\n",
       "           -2.2344e+00,  1.0312e+00]],\n",
       "\n",
       "         [[ 6.4087e-03, -7.8125e-03, -2.3956e-03,  ..., -3.2812e-01,\n",
       "            1.6406e+00,  6.4062e-01],\n",
       "          [ 1.4844e+00, -3.0000e+00,  2.1250e+00,  ..., -1.0859e+00,\n",
       "           -4.0312e+00,  7.6953e-01],\n",
       "          [-2.8906e+00, -3.0625e+00,  3.4375e+00,  ..., -1.1172e+00,\n",
       "           -4.6250e+00,  7.1875e-01],\n",
       "          [-5.0000e+00, -1.4531e+00,  4.7812e+00,  ..., -1.5547e+00,\n",
       "           -4.2500e+00,  4.1016e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-3.4027e-03,  2.0508e-02,  1.6113e-02,  ...,  8.4839e-03,\n",
       "           -3.1982e-02, -1.7395e-03],\n",
       "          [ 3.1738e-02, -1.4648e-01, -2.0020e-01,  ..., -3.3447e-02,\n",
       "            2.0410e-01,  3.4570e-01],\n",
       "          [ 1.0864e-02, -2.1973e-01,  1.1963e-01,  ..., -3.6328e-01,\n",
       "           -2.9663e-02,  3.3984e-01],\n",
       "          [ 1.4258e-01, -1.3965e-01, -1.4832e-02,  ...,  3.1128e-02,\n",
       "            1.7480e-01,  1.6556e-03]],\n",
       "\n",
       "         [[-2.4261e-03,  6.8283e-04,  7.3242e-03,  ..., -8.5449e-03,\n",
       "           -1.9989e-03,  4.5776e-03],\n",
       "          [-1.9531e-01, -1.4551e-01, -3.6377e-02,  ...,  2.6172e-01,\n",
       "            1.1230e-01,  1.0498e-01],\n",
       "          [-1.6895e-01,  2.0605e-01,  1.5625e-01,  ...,  1.6895e-01,\n",
       "            3.7500e-01, -3.4766e-01],\n",
       "          [-1.7578e-01, -1.2451e-01,  2.8906e-01,  ..., -2.0508e-01,\n",
       "           -2.1973e-02, -2.8711e-01]],\n",
       "\n",
       "         [[-9.6680e-02, -2.1851e-02,  4.0527e-02,  ..., -7.8125e-01,\n",
       "            1.1108e-02,  2.5024e-02],\n",
       "          [-1.2988e-01,  2.3828e-01, -2.8320e-01,  ...,  5.1953e-01,\n",
       "            2.1387e-01, -3.9062e-01],\n",
       "          [-1.4746e-01,  3.6133e-01, -5.6641e-02,  ...,  3.5547e-01,\n",
       "           -1.4648e-01, -4.6289e-01],\n",
       "          [-2.8516e-01, -1.0925e-02, -2.2705e-02,  ...,  5.1562e-01,\n",
       "           -2.2363e-01, -8.7891e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.6130e-04,  1.5640e-03, -5.4016e-03,  ...,  5.5313e-04,\n",
       "           -5.1270e-03, -3.3875e-03],\n",
       "          [ 1.6504e-01, -3.6523e-01,  3.2227e-01,  ...,  4.9316e-02,\n",
       "            2.1289e-01, -4.1406e-01],\n",
       "          [-4.8828e-01, -1.9043e-01,  2.1777e-01,  ..., -2.8906e-01,\n",
       "            8.2520e-02, -5.4688e-01],\n",
       "          [-4.0039e-01, -4.4336e-01,  2.5781e-01,  ..., -6.4453e-02,\n",
       "           -9.3262e-02, -2.9492e-01]],\n",
       "\n",
       "         [[ 2.6562e-01, -4.2419e-03, -5.2185e-03,  ...,  4.5776e-03,\n",
       "           -8.9645e-04,  4.3335e-03],\n",
       "          [-1.3086e-01, -3.1055e-01,  4.8242e-01,  ..., -2.0215e-01,\n",
       "            3.4180e-01, -1.0547e-01],\n",
       "          [-1.1353e-02,  1.0547e-01, -2.0020e-01,  ...,  2.8320e-01,\n",
       "            4.0039e-01, -1.6211e-01],\n",
       "          [ 9.2773e-02, -1.3245e-02,  1.4160e-01,  ...,  9.8267e-03,\n",
       "           -3.0664e-01, -4.1748e-02]],\n",
       "\n",
       "         [[-9.1553e-03, -5.3406e-04, -4.1199e-03,  ...,  8.8501e-03,\n",
       "            2.6550e-03, -2.5177e-03],\n",
       "          [ 1.8457e-01,  6.3965e-02, -3.8818e-02,  ...,  1.6797e-01,\n",
       "            1.8555e-01,  2.8198e-02],\n",
       "          [-1.2695e-01,  1.9629e-01, -5.0781e-01,  ..., -3.0396e-02,\n",
       "            1.9336e-01,  3.9844e-01],\n",
       "          [ 2.2705e-02, -5.2734e-01, -3.8477e-01,  ..., -4.3164e-01,\n",
       "           -3.2422e-01,  2.0020e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.5564e-03, -5.8594e-03,  2.6703e-03,  ..., -6.2988e-02,\n",
       "           -7.0312e-01, -5.3516e-01],\n",
       "          [ 1.8750e+00, -2.1875e+00, -1.4062e+00,  ...,  1.7734e+00,\n",
       "            1.0312e+00,  2.9844e+00],\n",
       "          [-1.7500e+00, -2.7656e+00, -2.7188e+00,  ...,  5.1953e-01,\n",
       "            2.5000e+00,  3.1094e+00],\n",
       "          [-4.0625e+00, -2.8594e+00, -2.8438e+00,  ...,  9.6875e-01,\n",
       "            3.7344e+00,  3.1562e+00]],\n",
       "\n",
       "         [[-1.5259e-02, -9.6436e-03, -6.4697e-03,  ..., -1.7871e-01,\n",
       "            4.1211e-01, -1.2598e-01],\n",
       "          [ 2.0117e-01, -2.2949e-01,  3.7109e-01,  ...,  8.7500e-01,\n",
       "           -1.2891e+00, -2.2656e+00],\n",
       "          [ 1.0391e+00, -6.8750e-01, -3.5156e-01,  ...,  4.8047e-01,\n",
       "           -1.9141e-01, -1.7969e+00],\n",
       "          [ 1.0234e+00, -1.3125e+00, -4.7363e-02,  ...,  3.6523e-01,\n",
       "           -5.5469e-01, -1.3281e+00]],\n",
       "\n",
       "         [[-2.6855e-03, -3.5706e-03, -3.0518e-03,  ..., -3.0078e-01,\n",
       "           -3.0859e-01, -8.2520e-02],\n",
       "          [ 1.2500e-01,  1.0625e+00, -1.7031e+00,  ...,  8.6328e-01,\n",
       "           -1.7891e+00,  6.8750e-01],\n",
       "          [ 1.0859e+00, -6.1719e-01, -1.1953e+00,  ..., -8.2031e-01,\n",
       "           -1.8203e+00, -6.0938e-01],\n",
       "          [ 1.6406e+00, -1.6875e+00, -2.0625e+00,  ...,  5.7031e-01,\n",
       "           -3.8125e+00,  4.5312e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.6528e-03, -6.3324e-04,  2.3041e-03,  ..., -1.7383e-01,\n",
       "            1.3438e+00,  7.7344e-01],\n",
       "          [-2.3594e+00, -1.7891e+00, -1.9688e+00,  ..., -1.0938e-01,\n",
       "           -6.2188e+00,  1.8516e+00],\n",
       "          [-1.9609e+00, -1.3906e+00, -9.8828e-01,  ...,  8.9062e-01,\n",
       "           -5.4062e+00,  8.8672e-01],\n",
       "          [-1.0938e-01,  8.4766e-01, -7.3438e-01,  ..., -1.1035e-01,\n",
       "           -6.2500e+00,  6.7578e-01]],\n",
       "\n",
       "         [[-7.2937e-03, -2.7466e-03, -3.8757e-03,  ...,  1.5391e+00,\n",
       "           -3.2812e-01,  1.0156e+00],\n",
       "          [ 1.0703e+00,  3.1445e-01,  1.6699e-01,  ..., -9.8047e-01,\n",
       "            2.1719e+00, -2.9375e+00],\n",
       "          [ 2.6406e+00,  2.3438e+00,  1.6016e+00,  ..., -1.5781e+00,\n",
       "           -2.1680e-01, -2.4062e+00],\n",
       "          [ 7.0312e-01,  1.4297e+00,  2.9375e+00,  ..., -2.0625e+00,\n",
       "            7.4609e-01, -2.9219e+00]],\n",
       "\n",
       "         [[ 1.2512e-03,  1.0132e-02, -7.4158e-03,  ...,  1.0156e+00,\n",
       "            1.2578e+00, -2.8687e-02],\n",
       "          [ 5.7422e-01, -9.2188e-01, -1.1875e+00,  ..., -8.2812e-01,\n",
       "           -2.7656e+00, -1.3281e+00],\n",
       "          [ 1.8984e+00, -2.1484e-02, -1.5078e+00,  ..., -1.6719e+00,\n",
       "           -2.8906e+00, -5.0000e-01],\n",
       "          [ 7.7734e-01, -6.6016e-01,  0.0000e+00,  ..., -2.6250e+00,\n",
       "           -2.0938e+00, -6.3672e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 1.0071e-02,  3.5400e-03,  3.2959e-02,  ...,  1.4343e-02,\n",
       "            3.5706e-03,  8.0078e-02],\n",
       "          [-7.9102e-02, -2.9053e-02,  9.7168e-02,  ..., -6.7578e-01,\n",
       "           -1.9238e-01,  5.9766e-01],\n",
       "          [-2.4219e-01,  2.5000e-01, -5.8350e-02,  ..., -7.5000e-01,\n",
       "           -1.9653e-02,  4.3164e-01],\n",
       "          [-4.1211e-01,  1.4844e-01, -2.6562e-01,  ..., -6.4844e-01,\n",
       "            2.1973e-01,  1.5918e-01]],\n",
       "\n",
       "         [[-2.7832e-02, -5.0659e-03,  1.8463e-03,  ...,  1.5503e-02,\n",
       "            6.3705e-04,  4.8523e-03],\n",
       "          [ 4.2383e-01, -8.6426e-02, -6.5234e-01,  ..., -3.1836e-01,\n",
       "           -1.2256e-01, -4.2383e-01],\n",
       "          [ 1.5234e-01,  2.7148e-01, -3.1445e-01,  ..., -1.4648e-02,\n",
       "           -1.8066e-01, -3.8086e-01],\n",
       "          [ 4.4727e-01,  7.4707e-02,  1.6406e-01,  ..., -8.4961e-02,\n",
       "           -1.2402e-01, -1.7578e-01]],\n",
       "\n",
       "         [[-5.3101e-03, -7.6294e-03, -1.9043e-02,  ..., -8.7891e-03,\n",
       "            2.1362e-02, -4.9438e-03],\n",
       "          [ 3.4570e-01, -1.0107e-01, -1.7969e-01,  ..., -3.0859e-01,\n",
       "            3.9453e-01, -1.3867e-01],\n",
       "          [ 1.1377e-01,  8.8867e-02, -5.5078e-01,  ...,  1.5137e-01,\n",
       "            2.2070e-01, -9.4238e-02],\n",
       "          [ 6.2988e-02,  2.0996e-01, -4.1809e-03,  ..., -1.1475e-01,\n",
       "            2.0410e-01, -5.3711e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9684e-03, -1.2634e-02,  1.7456e-02,  ..., -1.2207e-02,\n",
       "            1.3550e-02, -9.0942e-03],\n",
       "          [-3.7109e-01,  9.5703e-02,  4.4336e-01,  ...,  4.7119e-02,\n",
       "           -5.7812e-01,  3.5156e-02],\n",
       "          [ 4.9609e-01,  6.6016e-01,  2.6758e-01,  ..., -7.4707e-02,\n",
       "            3.8477e-01, -2.8906e-01],\n",
       "          [ 3.6621e-02,  3.1445e-01,  3.5156e-02,  ..., -1.2354e-01,\n",
       "           -1.0742e-01,  1.2402e-01]],\n",
       "\n",
       "         [[ 6.8359e-03, -1.9550e-04,  6.1798e-04,  ..., -8.4229e-03,\n",
       "            1.1841e-02, -3.4180e-03],\n",
       "          [-1.6504e-01,  2.3926e-01, -4.0039e-01,  ...,  1.3086e-01,\n",
       "           -9.7168e-02, -2.4609e-01],\n",
       "          [ 6.5918e-03,  5.1758e-02, -2.6172e-01,  ..., -2.6758e-01,\n",
       "           -2.1387e-01, -2.5977e-01],\n",
       "          [ 4.0820e-01,  8.2031e-02, -2.2949e-01,  ..., -7.2754e-02,\n",
       "           -1.4355e-01, -2.6953e-01]],\n",
       "\n",
       "         [[-2.7539e-01,  1.1368e-03, -3.5400e-03,  ..., -5.8594e-03,\n",
       "            4.8523e-03,  6.5308e-03],\n",
       "          [ 6.7578e-01,  4.0625e-01, -3.3984e-01,  ..., -2.5269e-02,\n",
       "            6.7578e-01, -2.5781e-01],\n",
       "          [ 5.8594e-01,  4.1211e-01, -4.8584e-02,  ...,  4.3750e-01,\n",
       "            2.5586e-01, -2.4512e-01],\n",
       "          [ 9.0625e-01,  3.0273e-01,  3.5742e-01,  ..., -1.2158e-01,\n",
       "            1.8262e-01,  4.1016e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0620e-02,  1.0757e-03, -8.8120e-04,  ...,  7.2327e-03,\n",
       "           -1.0859e+00,  7.4219e-01],\n",
       "          [-3.1641e-01,  4.3164e-01, -5.1562e-01,  ..., -3.1406e+00,\n",
       "            3.0781e+00, -1.2734e+00],\n",
       "          [-1.4648e-01,  6.5430e-02, -5.1172e-01,  ..., -3.3906e+00,\n",
       "            4.0625e+00, -2.3906e+00],\n",
       "          [-3.3203e-01,  3.7891e-01, -3.4766e-01,  ..., -3.5156e+00,\n",
       "            4.1562e+00, -2.5312e+00]],\n",
       "\n",
       "         [[ 2.3346e-03, -9.7656e-03,  5.3406e-04,  ..., -3.4180e-01,\n",
       "            1.1641e+00, -3.8574e-02],\n",
       "          [ 4.2188e-01,  2.1484e-01, -5.8594e-01,  ...,  1.4453e+00,\n",
       "           -1.4453e+00, -1.6250e+00],\n",
       "          [ 1.9375e+00,  9.6875e-01, -3.2422e-01,  ...,  1.6016e+00,\n",
       "           -2.7500e+00, -1.6797e-01],\n",
       "          [ 2.1094e+00,  2.2969e+00, -2.6953e-01,  ...,  2.7031e+00,\n",
       "           -2.6094e+00,  6.7188e-01]],\n",
       "\n",
       "         [[ 1.2817e-03,  1.5717e-03,  2.8381e-03,  ..., -1.1094e+00,\n",
       "           -1.2188e+00,  1.1328e+00],\n",
       "          [-2.8125e+00,  2.1250e+00,  2.5469e+00,  ...,  4.7500e+00,\n",
       "            1.6875e+00,  9.8047e-01],\n",
       "          [-4.0938e+00,  1.0078e+00,  4.5312e+00,  ...,  3.8281e+00,\n",
       "            1.6406e+00,  1.0986e-01],\n",
       "          [-1.6562e+00, -1.2500e+00,  4.1250e+00,  ...,  3.2656e+00,\n",
       "            2.2656e+00, -1.4355e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.8992e-03,  1.8539e-03, -2.9755e-03,  ...,  6.9141e-01,\n",
       "           -2.1973e-01,  9.6094e-01],\n",
       "          [ 9.2285e-02, -1.4258e-01, -7.9688e-01,  ..., -2.3906e+00,\n",
       "            1.2188e+00, -2.1406e+00],\n",
       "          [-4.8438e-01,  7.6172e-02, -1.4453e-01,  ..., -2.0000e+00,\n",
       "            1.1406e+00, -2.0625e+00],\n",
       "          [-3.0273e-01,  7.6660e-02, -1.3086e-01,  ..., -3.7969e+00,\n",
       "           -1.4453e+00, -2.9219e+00]],\n",
       "\n",
       "         [[ 1.0620e-02, -2.0504e-04,  6.3705e-04,  ...,  2.9297e-01,\n",
       "           -5.7031e-01,  4.7266e-01],\n",
       "          [ 8.4375e+00,  3.9844e+00, -2.2969e+00,  ...,  1.1016e+00,\n",
       "           -1.2344e+00,  1.1328e+00],\n",
       "          [ 5.0000e-01,  3.9688e+00,  9.3750e-02,  ...,  4.3164e-01,\n",
       "           -7.6562e-01,  1.1641e+00],\n",
       "          [-9.6250e+00,  2.4062e+00,  1.9141e-01,  ...,  5.8594e-01,\n",
       "           -1.9434e-01, -2.2266e-01]],\n",
       "\n",
       "         [[-3.5553e-03,  2.4872e-03, -9.3842e-04,  ..., -2.0156e+00,\n",
       "           -2.6367e-01, -1.9844e+00],\n",
       "          [-6.4062e-01, -1.1016e+00,  2.0625e+00,  ...,  1.5156e+00,\n",
       "           -7.4219e-01,  1.6094e+00],\n",
       "          [ 2.7188e+00, -1.7031e+00,  1.5469e+00,  ...,  2.9219e+00,\n",
       "           -1.1719e+00,  2.5469e+00],\n",
       "          [ 3.5156e+00, -2.7969e+00,  9.3750e-01,  ...,  3.7656e+00,\n",
       "           -8.5547e-01,  3.3750e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.5564e-03,  6.7520e-04,  2.9419e-02,  ...,  2.7618e-03,\n",
       "           -5.8838e-02,  3.7994e-03],\n",
       "          [ 8.0469e-01,  4.3945e-01,  7.8125e-01,  ..., -1.0254e-02,\n",
       "           -6.4453e-02, -8.3203e-01],\n",
       "          [ 1.2793e-01,  2.9883e-01,  2.7344e-01,  ...,  6.4453e-01,\n",
       "            1.2578e+00, -2.6172e-01],\n",
       "          [ 4.4141e-01,  2.3926e-01,  7.4707e-02,  ...,  7.5781e-01,\n",
       "            5.4688e-01, -4.7070e-01]],\n",
       "\n",
       "         [[-2.5024e-03,  1.4038e-03,  2.9449e-03,  ...,  6.1646e-03,\n",
       "            2.7161e-03, -7.6294e-03],\n",
       "          [ 2.0410e-01,  1.9824e-01,  1.9434e-01,  ..., -1.7773e-01,\n",
       "            2.3438e-01, -1.3672e-01],\n",
       "          [-4.0771e-02, -7.5378e-03,  4.5410e-02,  ..., -1.1133e-01,\n",
       "            1.0596e-01, -6.8848e-02],\n",
       "          [-1.3574e-01,  2.8711e-01,  4.1211e-01,  ..., -8.0566e-02,\n",
       "           -1.6357e-02, -4.3030e-03]],\n",
       "\n",
       "         [[-5.3467e-02,  1.8539e-03,  7.1106e-03,  ...,  1.0538e-04,\n",
       "           -4.5471e-03,  1.1536e-02],\n",
       "          [-1.4844e-01, -5.0000e-01,  7.6660e-02,  ..., -2.3828e-01,\n",
       "           -2.1875e-01, -3.6377e-02],\n",
       "          [-3.2959e-02, -9.8633e-02,  8.3496e-02,  ..., -5.5908e-02,\n",
       "           -2.6953e-01,  7.4707e-02],\n",
       "          [-5.1562e-01,  2.3926e-01, -3.0859e-01,  ...,  7.9590e-02,\n",
       "           -8.1250e-01,  1.3965e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.8147e-03,  1.1475e-02, -7.1335e-04,  ...,  7.9956e-03,\n",
       "           -6.7444e-03,  3.7689e-03],\n",
       "          [-3.0078e-01,  6.1768e-02,  8.4839e-03,  ...,  4.8438e-01,\n",
       "            2.4121e-01, -1.4453e-01],\n",
       "          [ 3.3936e-02, -3.1445e-01,  4.7607e-02,  ...,  5.3516e-01,\n",
       "            3.6865e-02, -2.6562e-01],\n",
       "          [ 1.2061e-01,  1.5527e-01, -7.4219e-02,  ...,  2.5781e-01,\n",
       "            5.4321e-03,  1.0107e-01]],\n",
       "\n",
       "         [[ 2.0630e-02,  3.1128e-03, -1.3733e-02,  ...,  2.0996e-02,\n",
       "            1.1902e-02,  6.5613e-04],\n",
       "          [-3.3984e-01,  2.3193e-02,  4.0234e-01,  ...,  4.0430e-01,\n",
       "           -3.8818e-02,  2.1289e-01],\n",
       "          [-7.5000e-01,  2.4512e-01,  2.9688e-01,  ...,  3.5938e-01,\n",
       "           -3.7891e-01, -3.0273e-01],\n",
       "          [-8.8281e-01, -2.7734e-01,  2.5195e-01,  ..., -2.7930e-01,\n",
       "           -2.1094e-01, -1.5820e-01]],\n",
       "\n",
       "         [[-5.7068e-03, -1.0742e-02,  6.6528e-03,  ...,  4.7302e-03,\n",
       "            2.7466e-03, -8.1177e-03],\n",
       "          [-1.8555e-02,  1.0938e-01, -3.6328e-01,  ...,  5.3516e-01,\n",
       "           -1.1865e-01, -9.8267e-03],\n",
       "          [-3.3398e-01, -1.9629e-01,  7.7637e-02,  ..., -3.6133e-02,\n",
       "           -2.9297e-01,  1.8262e-01],\n",
       "          [-7.7637e-02,  1.2256e-01,  2.6953e-01,  ..., -9.4238e-02,\n",
       "            1.7090e-01,  1.5625e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.2534e-04,  1.2512e-03,  1.0681e-02,  ...,  9.5703e-02,\n",
       "            7.6172e-01, -1.1641e+00],\n",
       "          [ 5.7812e-01, -5.8594e-02,  8.9062e-01,  ..., -5.8984e-01,\n",
       "           -2.2500e+00,  2.6406e+00],\n",
       "          [ 1.9688e+00,  1.1406e+00,  1.4375e+00,  ..., -1.1953e+00,\n",
       "           -1.9219e+00,  3.0156e+00],\n",
       "          [ 1.5469e+00,  1.7188e+00,  1.6797e+00,  ..., -2.5469e+00,\n",
       "           -7.5781e-01,  3.4531e+00]],\n",
       "\n",
       "         [[-8.0566e-03, -6.5002e-03, -2.7161e-03,  ...,  7.1094e-01,\n",
       "            7.0801e-02,  2.9102e-01],\n",
       "          [-1.2422e+00,  4.7656e-01, -1.5625e-02,  ...,  8.2422e-01,\n",
       "           -4.4531e-01,  4.6680e-01],\n",
       "          [ 8.0859e-01, -1.4297e+00, -2.1562e+00,  ...,  8.6328e-01,\n",
       "            1.3594e+00,  5.8594e-01],\n",
       "          [ 3.2344e+00, -3.4375e+00, -3.1875e+00,  ...,  5.3906e-01,\n",
       "            4.5898e-01,  2.1094e+00]],\n",
       "\n",
       "         [[ 2.8076e-03,  4.0283e-03, -9.1171e-04,  ...,  4.7852e-01,\n",
       "           -3.6914e-01, -8.3203e-01],\n",
       "          [ 7.6172e-01, -1.4531e+00,  1.1562e+00,  ...,  4.9805e-01,\n",
       "           -7.3828e-01,  3.7656e+00],\n",
       "          [ 1.8438e+00, -6.4844e-01,  9.5312e-01,  ...,  1.3770e-01,\n",
       "           -4.7266e-01,  2.8438e+00],\n",
       "          [ 8.6719e-01, -3.8281e-01,  6.0938e-01,  ...,  7.2266e-01,\n",
       "            2.2852e-01,  2.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1047e-02,  6.2561e-03, -1.5991e-02,  ..., -5.0781e-01,\n",
       "            5.8105e-02,  1.2422e+00],\n",
       "          [-6.5625e-01, -9.4922e-01, -9.0625e-01,  ...,  1.8750e+00,\n",
       "            1.1328e+00, -1.7422e+00],\n",
       "          [-2.3438e-01,  3.4180e-02, -1.3125e+00,  ...,  1.6328e+00,\n",
       "           -1.2969e+00, -3.6094e+00],\n",
       "          [ 1.1035e-01,  2.9297e-01, -1.1562e+00,  ...,  1.8047e+00,\n",
       "           -5.3906e-01, -3.5312e+00]],\n",
       "\n",
       "         [[-5.3711e-03, -9.2316e-04, -3.6926e-03,  ..., -5.6885e-02,\n",
       "            3.5742e-01, -1.9141e+00],\n",
       "          [ 1.3672e+00, -1.4062e+00, -3.9844e-01,  ..., -1.9844e+00,\n",
       "           -2.4023e-01,  3.0156e+00],\n",
       "          [-1.1953e+00, -3.4844e+00, -1.8203e+00,  ...,  6.1328e-01,\n",
       "           -2.9492e-01,  2.6719e+00],\n",
       "          [-3.1250e+00, -4.0312e+00, -2.6250e+00,  ..., -1.0205e-01,\n",
       "            1.2969e+00,  3.6250e+00]],\n",
       "\n",
       "         [[ 1.5320e-02, -9.7046e-03, -8.2397e-03,  ...,  1.0254e-01,\n",
       "            6.7578e-01,  1.0469e+00],\n",
       "          [-2.4512e-01,  7.4707e-02,  2.5391e-02,  ..., -4.1562e+00,\n",
       "           -7.6562e-01, -2.6562e+00],\n",
       "          [-4.3750e-01, -1.7578e-01,  3.1250e-01,  ..., -1.8984e+00,\n",
       "           -1.9453e+00, -3.0312e+00],\n",
       "          [-2.6758e-01, -5.0537e-02,  2.3242e-01,  ..., -1.8281e+00,\n",
       "           -1.9375e+00, -3.5312e+00]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 4.6082e-03,  4.5166e-03,  2.5787e-03,  ..., -5.9204e-03,\n",
       "           -1.8616e-03,  1.0498e-02],\n",
       "          [ 5.3516e-01, -3.1641e-01, -4.6680e-01,  ...,  6.5918e-02,\n",
       "            4.7461e-01, -4.8340e-02],\n",
       "          [-8.1250e-01, -1.3867e-01,  6.2012e-02,  ..., -1.7090e-01,\n",
       "            3.8281e-01,  1.7480e-01],\n",
       "          [ 2.7344e-02, -6.6016e-01, -3.8281e-01,  ..., -5.2734e-01,\n",
       "            1.4746e-01,  8.6914e-02]],\n",
       "\n",
       "         [[ 9.9121e-02,  1.7822e-02, -7.2937e-03,  ..., -5.2002e-02,\n",
       "           -2.3804e-02,  3.5645e-02],\n",
       "          [-3.8281e-01, -3.4180e-01, -1.5625e-01,  ...,  4.5508e-01,\n",
       "            1.9824e-01,  3.1445e-01],\n",
       "          [ 5.0781e-02,  1.4062e-01, -9.2316e-04,  ...,  2.8906e-01,\n",
       "            1.3184e-01, -2.5000e-01],\n",
       "          [ 2.3047e-01, -1.2695e-01,  3.9844e-01,  ...,  2.3730e-01,\n",
       "           -4.3750e-01, -1.4355e-01]],\n",
       "\n",
       "         [[ 1.5030e-03, -2.7008e-03, -1.3281e-01,  ...,  4.1504e-03,\n",
       "           -5.1880e-03, -3.3569e-03],\n",
       "          [ 4.2188e-01,  7.0801e-02,  6.4062e-01,  ...,  1.1797e+00,\n",
       "           -4.7070e-01, -4.0234e-01],\n",
       "          [ 3.6914e-01,  4.0820e-01, -7.9102e-02,  ...,  8.0859e-01,\n",
       "            2.6978e-02, -1.4062e-01],\n",
       "          [-9.1309e-02, -3.7109e-01,  4.7461e-01,  ...,  3.5547e-01,\n",
       "            2.7539e-01,  4.5312e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.2185e-03, -1.0254e-02, -9.8267e-03,  ...,  2.9602e-03,\n",
       "           -6.9885e-03, -3.4332e-03],\n",
       "          [ 6.5234e-01, -2.8125e-01,  3.2617e-01,  ..., -7.0312e-01,\n",
       "            1.3672e-01,  2.4902e-01],\n",
       "          [ 8.8379e-02, -2.8125e-01, -3.0859e-01,  ..., -9.4531e-01,\n",
       "           -1.1475e-01, -6.6797e-01],\n",
       "          [ 4.7461e-01,  1.1621e-01, -1.0400e-01,  ..., -2.3438e-01,\n",
       "            6.4062e-01,  5.5469e-01]],\n",
       "\n",
       "         [[-2.2339e-02, -1.5564e-02, -4.1504e-02,  ..., -3.2227e-02,\n",
       "           -2.7588e-02,  6.3281e-01],\n",
       "          [ 3.3008e-01,  2.4414e-01,  1.7480e-01,  ..., -1.6357e-02,\n",
       "           -1.7871e-01, -7.4219e-01],\n",
       "          [-2.3535e-01, -8.6426e-02,  4.9805e-01,  ..., -2.2461e-01,\n",
       "            2.0117e-01, -4.0039e-01],\n",
       "          [-2.7148e-01, -1.5991e-02, -3.0078e-01,  ..., -3.0078e-01,\n",
       "            6.0547e-01, -4.9609e-01]],\n",
       "\n",
       "         [[-1.0315e-02, -1.2741e-03, -6.3477e-03,  ...,  4.0283e-03,\n",
       "           -8.7891e-03,  1.1475e-02],\n",
       "          [-1.2344e+00,  1.5527e-01, -2.5195e-01,  ..., -5.6641e-01,\n",
       "            1.7285e-01,  1.9434e-01],\n",
       "          [-7.3242e-02,  5.0000e-01,  4.1602e-01,  ..., -5.9766e-01,\n",
       "           -6.6895e-02,  8.3008e-02],\n",
       "          [-4.1992e-01,  3.4375e-01,  4.7363e-02,  ..., -3.4180e-01,\n",
       "           -5.3125e-01, -1.8652e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-5.3711e-03,  1.4420e-03,  1.6113e-02,  ...,  4.0430e-01,\n",
       "           -5.0000e-01,  7.4609e-01],\n",
       "          [ 2.2500e+00,  1.5547e+00,  2.0938e+00,  ..., -1.6602e-02,\n",
       "            1.0938e+00,  5.8594e-01],\n",
       "          [ 4.4062e+00,  8.7500e-01,  9.1016e-01,  ..., -6.2891e-01,\n",
       "            7.4219e-01,  2.5000e-01],\n",
       "          [ 9.2188e-01, -8.7500e-01, -5.1172e-01,  ..., -6.0156e-01,\n",
       "            6.0547e-01,  9.5312e-01]],\n",
       "\n",
       "         [[ 5.4016e-03, -1.2894e-03, -5.6076e-04,  ..., -7.9956e-03,\n",
       "           -1.3594e+00,  2.2500e+00],\n",
       "          [-1.8594e+00, -7.1094e-01, -5.8594e-02,  ..., -2.1562e+00,\n",
       "            2.1875e+00, -5.4375e+00],\n",
       "          [-2.0625e+00, -1.3750e+00, -1.7578e-01,  ..., -2.4531e+00,\n",
       "            3.0938e+00, -3.3125e+00],\n",
       "          [ 1.2695e-01, -2.3281e+00,  6.0938e-01,  ..., -5.5625e+00,\n",
       "            3.7656e+00, -2.2344e+00]],\n",
       "\n",
       "         [[-1.1902e-03, -7.0801e-03,  1.2207e-03,  ...,  1.1094e+00,\n",
       "           -5.5469e-01, -1.0000e+00],\n",
       "          [-2.3438e+00, -3.1250e+00, -2.0469e+00,  ...,  2.4707e-01,\n",
       "            2.0215e-01,  1.5430e-01],\n",
       "          [ 2.0469e+00, -1.1875e+00, -1.5625e-02,  ...,  1.2031e+00,\n",
       "           -3.3789e-01, -6.0156e-01],\n",
       "          [ 4.7188e+00,  1.1250e+00,  1.3438e+00,  ...,  3.3398e-01,\n",
       "           -3.5742e-01, -5.5469e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.3417e-03, -5.2185e-03,  2.5330e-03,  ...,  8.2031e-02,\n",
       "           -8.8672e-01, -4.7461e-01],\n",
       "          [ 2.8750e+00,  4.7266e-01,  6.0156e-01,  ...,  2.0000e+00,\n",
       "            1.1641e+00,  6.0547e-01],\n",
       "          [ 2.2188e+00,  1.2500e+00, -1.4219e+00,  ...,  2.4375e+00,\n",
       "            3.5156e+00,  7.1875e-01],\n",
       "          [-5.4297e-01,  2.6719e+00, -2.3730e-01,  ...,  1.1250e+00,\n",
       "            2.2656e+00,  1.5625e+00]],\n",
       "\n",
       "         [[-4.5166e-03, -2.3346e-03,  7.8735e-03,  ..., -1.7422e+00,\n",
       "            2.4609e-01, -1.8438e+00],\n",
       "          [-1.6719e+00,  2.0469e+00,  2.6094e+00,  ...,  4.2188e+00,\n",
       "           -1.0234e+00,  1.4766e+00],\n",
       "          [-3.6406e+00, -1.2109e-01,  1.3438e+00,  ...,  1.8672e+00,\n",
       "            9.2969e-01,  4.0938e+00],\n",
       "          [-7.2266e-01, -1.3594e+00, -2.5781e-01,  ...,  2.6875e+00,\n",
       "            5.8594e-01,  3.3125e+00]],\n",
       "\n",
       "         [[-9.2773e-03, -2.2888e-03, -2.1210e-03,  ..., -9.9219e-01,\n",
       "            1.5527e-01,  2.7148e-01],\n",
       "          [ 1.9727e-01, -3.0078e-01,  3.5547e-01,  ...,  1.4375e+00,\n",
       "            1.7500e+00, -8.0078e-01],\n",
       "          [-3.7500e-01,  6.3477e-02,  3.7109e-01,  ...,  2.3906e+00,\n",
       "            1.7969e-01, -8.0859e-01],\n",
       "          [-1.7969e-01,  5.5664e-02,  5.2344e-01,  ...,  3.2188e+00,\n",
       "           -1.3125e+00,  9.2578e-01]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 1.3428e-02, -8.4229e-03, -1.5747e-02,  ...,  1.8188e-02,\n",
       "            1.5991e-02,  6.3782e-03],\n",
       "          [-9.4238e-02,  4.4336e-01, -6.7871e-02,  ..., -3.0469e-01,\n",
       "           -2.4902e-01, -2.8442e-02],\n",
       "          [ 1.1658e-02, -1.9922e-01,  9.0332e-02,  ..., -6.8359e-01,\n",
       "           -3.8672e-01,  7.8613e-02],\n",
       "          [-3.4912e-02,  3.7500e-01, -1.8945e-01,  ..., -1.0254e-01,\n",
       "           -2.2461e-01, -4.6484e-01]],\n",
       "\n",
       "         [[-1.5030e-03, -8.9722e-03,  1.7548e-03,  ...,  3.0670e-03,\n",
       "            7.0801e-03,  6.9427e-04],\n",
       "          [ 5.6250e-01,  1.5527e-01,  1.7676e-01,  ..., -1.1279e-01,\n",
       "           -7.4219e-01, -7.3828e-01],\n",
       "          [ 2.8711e-01, -3.8574e-02,  2.3926e-02,  ...,  5.3125e-01,\n",
       "           -6.7578e-01,  6.1279e-02],\n",
       "          [ 1.3379e-01, -1.8652e-01,  1.7188e-01,  ..., -8.3008e-03,\n",
       "           -5.6250e-01,  8.7109e-01]],\n",
       "\n",
       "         [[ 5.0537e-02,  1.5527e-01,  3.1586e-03,  ..., -3.3203e-02,\n",
       "            4.4336e-01,  3.4180e-03],\n",
       "          [ 1.1816e-01,  2.2754e-01, -6.2500e-02,  ..., -1.0254e-01,\n",
       "           -1.1250e+00, -4.6094e-01],\n",
       "          [ 2.8906e-01,  3.2227e-01,  2.2461e-01,  ..., -5.8350e-02,\n",
       "           -1.2891e+00,  3.5547e-01],\n",
       "          [-8.3008e-03,  6.8359e-01,  4.9561e-02,  ..., -2.0117e-01,\n",
       "           -1.6016e+00, -3.1641e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.9885e-03, -1.1780e-02,  2.6550e-03,  ...,  6.8970e-03,\n",
       "            7.1716e-03, -6.2256e-03],\n",
       "          [ 2.0410e-01, -2.2852e-01,  7.0801e-02,  ...,  4.5703e-01,\n",
       "           -2.9297e-01,  4.1211e-01],\n",
       "          [-5.7812e-01, -2.6172e-01, -4.8828e-01,  ..., -1.0596e-01,\n",
       "           -5.6885e-02,  1.7773e-01],\n",
       "          [-1.0791e-01, -9.2773e-02, -5.2246e-02,  ...,  3.4668e-02,\n",
       "           -8.6914e-02, -4.6484e-01]],\n",
       "\n",
       "         [[ 3.7354e-02,  1.3574e-01, -1.1084e-01,  ..., -2.2705e-02,\n",
       "            2.0630e-02,  5.5908e-02],\n",
       "          [ 3.8281e-01, -1.5859e+00,  1.6479e-02,  ..., -1.5820e-01,\n",
       "           -1.3379e-01,  6.7969e-01],\n",
       "          [ 1.4551e-01,  4.9072e-02, -1.2207e-01,  ..., -1.2891e-01,\n",
       "            3.0859e-01, -5.6152e-02],\n",
       "          [-1.5039e-01, -1.2451e-01,  7.8516e-01,  ...,  2.4316e-01,\n",
       "            4.2578e-01,  1.6992e-01]],\n",
       "\n",
       "         [[-7.2632e-03,  1.3062e-02,  3.3112e-03,  ...,  2.4414e-03,\n",
       "            7.6904e-03, -9.2163e-03],\n",
       "          [ 9.2969e-01, -1.2578e+00,  4.7119e-02,  ...,  9.0820e-02,\n",
       "            2.8516e-01,  2.4531e+00],\n",
       "          [ 8.3984e-01, -4.7656e-01, -8.2031e-02,  ..., -4.7461e-01,\n",
       "            2.5391e-01,  6.3281e-01],\n",
       "          [-2.8906e-01,  5.8203e-01, -3.9551e-02,  ..., -5.0391e-01,\n",
       "           -7.1484e-01, -9.8145e-02]]]], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6e71a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape\n",
    "# 4 is the number of token in the input text i.e. \"input_ids\", here its \"Hello how are\"\n",
    "# 128256 is the vocab size of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b0297c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3750, device='mps:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits[0,-1][12800]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa88825",
   "metadata": {},
   "source": [
    "Logits are the raw, unnormalized scores that the model produces to represent the model's confidence for the next word in the input sequence\n",
    "\n",
    "Softmax operation converts the logits into a probability distribution, i.e. what is the probability of a given word/subword to be the next token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5e4b2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karnavivek/askmyprofession/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "proba_dist = nn.Softmax()(out.logits[0, -1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63b5414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e60ea144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9514"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('you') #or we can use the code: tokenizer.vocab['you']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4754315",
   "metadata": {},
   "source": [
    "'you' - This is nothing but ' you' -  yes there is a space before 'you'!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3fd3f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.6250, device='mps:0', grad_fn=<SelectBackward0>)\n",
      "Probability of next word being \"you\" is 0.987900972366333\n"
     ]
    }
   ],
   "source": [
    "print(out.logits[0, -1][499])\n",
    "print(f'Probability of next word being \"you\" is {proba_dist[499]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5e514",
   "metadata": {},
   "source": [
    "'you' & ' you' are considered different, thats why if you check, the probability of 'you' being the next word, it will be less than 'you':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e38dfa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.1250, device='mps:0', grad_fn=<SelectBackward0>)\n",
      "Probability of next word being \"you\" is 3.6815642943111015e-06\n"
     ]
    }
   ],
   "source": [
    "print(out.logits[0, -1][9514])\n",
    "print(f'Probability of next word being \"you\" is {proba_dist[9514]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ea270",
   "metadata": {},
   "source": [
    "As you can see its verrrryyy less, compared to \"you\" which has 98% surity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13d2000f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16309,    11,   527,   499]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['logits'].argmax(axis=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5599501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' you'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PREDICTING THE NEXT WORD\n",
    "tokenizer.decode(out['logits'].argmax(dim=-1)[0, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493d1e4",
   "metadata": {},
   "source": [
    "Now, if you run this in a loop, thats how the next word is getting generated!\n",
    "\n",
    "i.e. \n",
    "- Loop 1: \n",
    "Input 1: \"Hello how are\"\n",
    "Output 1: \" you\"\n",
    "\n",
    "- Loop 2:\n",
    "Input 1 + Output 1 = \"Hello how are you\"\n",
    "Output 2: \" today\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32127ebf",
   "metadata": {},
   "source": [
    "### Training on Sequence (Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c5a94",
   "metadata": {},
   "source": [
    "To FineTune LLMs, the goal here is to give it input sentences & make it learn sequence, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a226f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,    791,  89124,    315,    420,   2447,    374,  66954,     74,\n",
      "            735,  40315]])\n",
      "['<|begin_of_text|>The Programmer of this project is Vivek Karna']\n"
     ]
    }
   ],
   "source": [
    "#if we want our LLM to learn:\n",
    "sentence = [\"The Programmer of this project is Vivek Karna\"]\n",
    "tokenized = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "print(tokenized)\n",
    "print(tokenizer.batch_decode(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb891b4",
   "metadata": {},
   "source": [
    "From the above \"tokenized\" sequence, we will produce INPUT sequence & OUTPUT sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac26e154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:  tensor([[128000,    791,  89124,    315,    420,   2447,    374,  66954,     74,\n",
      "            735]])\n",
      "Target Sequence:  tensor([[  791, 89124,   315,   420,  2447,   374, 66954,    74,   735, 40315]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized[:, :-1]  # Input sequence (all tokens except the last one)\n",
    "target_ids = tokenized[:, 1:]  # Target sequence (all tokens except the first one)\n",
    "\n",
    "print(\"Input Sequence: \", input_ids)\n",
    "print(\"Target Sequence: \", target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad88d2",
   "metadata": {},
   "source": [
    "This means that for input id [128000], we want our transformer to predict [791], so on & so forth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd03f6",
   "metadata": {},
   "source": [
    "We can do the same with the chat template approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09a9d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Capital of India?<|eot_id|><|start_header_id|>assitant<|end_header_id|>\n",
      "\n",
      "Capital:<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "question = \"Capital of India?\"\n",
    "answer = \"New Delhi\"\n",
    "\n",
    "prompt_ = [\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Capital of India?\"},\n",
    "\n",
    "    {\"role\": \"assitant\",\n",
    "     \"content\": \"Capital: \"}\n",
    "]\n",
    "answer = \"New Delhi\"\n",
    "\n",
    "chat_template = tokenizer.apply_chat_template(prompt_, continue_final_message=True, tokenize=False)\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11fa3826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Capital of India?<|eot_id|><|start_header_id|>assitant<|end_header_id|>\n",
      "\n",
      "Capital:<|eot_id|> New Delhi<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "full_response_text = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "print(full_response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4ecc9",
   "metadata": {},
   "source": [
    "We want the model to learn the the above text, i.e. with chat template + answer, this is nothing but prompt-answer pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4380376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   6890,     30, 128009, 128006,\n",
      "            395,  52044, 128007,    271,  64693,     25, 128009,   1561,  22767,\n",
      "         128009]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(full_response_text, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a48705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:  tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   6890,     30, 128009, 128006,\n",
      "            395,  52044, 128007,    271,  64693,     25, 128009,   1561,  22767]])\n",
      "Target Sequence:  tensor([[128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,     25,\n",
      "           6790,    220,   2366,     18,    198,  15724,   2696,     25,    220,\n",
      "           1627,  10263,    220,   2366,     19,    271, 128009, 128006,    882,\n",
      "         128007,    271,  64693,    315,   6890,     30, 128009, 128006,    395,\n",
      "          52044, 128007,    271,  64693,     25, 128009,   1561,  22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized[:, :-1] # Start from the first token to the second last token\n",
    "target_ids = tokenized[:, 1:] # Start from the second token to the last token\n",
    "\n",
    "print(\"Input Sequence: \", input_ids)\n",
    "print(\"Target Sequence: \", target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c562ce9",
   "metadata": {},
   "source": [
    "So for finetuning, one of the ways is that you need to make a prompt-answer structure -> tokenize -> Learn it -> calculate the loss function -> use optimizers to reduce the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f0b876",
   "metadata": {},
   "source": [
    "While finetuning a pretrained model, ideally we should be applying the loss over our answer and not the entire sequence (which also includes the prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb4cb4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delhi'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1561)\n",
    "tokenizer.convert_ids_to_tokens(22767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666a293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1561, 22767]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tokenized = tokenizer([\" \" + answer], add_special_tokens=False, return_tensors='pt')['input_ids']\n",
    "labels_tokenized\n",
    "#not only this but we end token to also be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb17eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids.shape[1] #this is max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1671cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tokenized = tokenizer([\" \" + answer + tokenizer.eos_token], add_special_tokens=False, return_tensors='pt', padding=\"max_length\", max_length=target_ids.shape[1])[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb34e86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009,   1561,  22767, 128009]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tokenized\n",
    "# here the target ids is prepended with padding token, padding tokens as said is the eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc1e37",
   "metadata": {},
   "source": [
    "We will now convert all the above padding tokens (128009) into -100, why? well llama documentation says so, its helps in masking while calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b428d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  1561, 22767,  -100]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100) #torch.where(condition, x, y) -> X when condition is True, Y when condition is False\n",
    "labels_tokenized_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525515f",
   "metadata": {},
   "source": [
    "Now, lets combine all of this into 1 function where we will input: prompt, target_responses; output: input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target(prompt, target_responses):\n",
    "    chat_template = tokenizer.apply_chat_template(prompt_, continue_final_message=True, tokenize=False)\n",
    "    full_response_text = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "    tokenized = tokenizer(full_response_text, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
    "    input_ids = tokenized[:, :-1] # Start from the first token to the second last token\n",
    "    target_ids = tokenized[:, 1:] # Start from the second token to the last token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
