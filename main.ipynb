{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f013060",
   "metadata": {},
   "source": [
    "Building the base LLM Model which will be then used for finetuning LoRA + Quantization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3aedda",
   "metadata": {},
   "source": [
    "choosing base model to do our job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068bc6c6",
   "metadata": {},
   "source": [
    "what is the criteria of selection of a base LLM?\n",
    "- we are going to start with Meta's llama 3.2 1B Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cc357",
   "metadata": {},
   "source": [
    "## Loading Models and Basic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d22a5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karnavivek/askmyprofession/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1b-Instruct\"\n",
    "device = 'mps' # Use 'cuda' for GPU, 'mps' for Mac, or 'cpu' for CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bddd7",
   "metadata": {},
   "source": [
    "before going to the next step, make sure you are logged in through huggingface in web & have created \"access token\", only when you do that, we can access authorized models from the website.\n",
    "\n",
    "after generating & copying access token from huggingface website,\n",
    "go to vscode terminal & type -> \"huggingface-cli login\"\n",
    "\n",
    "then paste your access token,\n",
    "\n",
    "then you can run the below code! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4b4a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token as EOS token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=device)\n",
    "#took 48.2s to download the model and load llama 3.2 1b instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2874cfc",
   "metadata": {},
   "source": [
    "CausalLM predict the NEXT WORD given a prefix of sentence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b28e0d",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d0a74a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the capital of India? New Delhi\\nThe capital of India'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_pipeline = pipeline(task='text-generation',\n",
    "                             model=model,\n",
    "                             tokenizer=tokenizer)\n",
    "\n",
    "generate_pipeline(\"What is the capital of India?\", max_new_tokens=7)\n",
    "#max_new_tokens = number of words to be generated in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e1243",
   "metadata": {},
   "source": [
    "##### Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22b91f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'What is the capital of India? New Delhi?\\nNo, it is not New Delhi. New Delhi is the capital of India, but the capital of India is'}],\n",
       " [{'generated_text': 'What is the capital of USA? Washington\\nThe capital of the United States of America is Washington D.C. (short for District of Columbia). It is located'}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_pipeline([\"What is the capital of India?\", \n",
    "                   \"What is the capital of USA?\"], max_new_tokens=25)\n",
    "#If list of sentences is given, The max_new_tokens is applied to each sentence in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044d6f8",
   "metadata": {},
   "source": [
    "#### What is happening inside the Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21d2342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   3923,    374,    279,   6864,    315,   6890,     30],\n",
      "        [128000,   3923,    374,    279,   6864,    315,   7427,     30]],\n",
      "       device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "input_prompt = [\"What is the capital of India?\",\n",
    "                \"What is the capital of USA?\"]\n",
    "\n",
    "#tokeinzers convert the input string into list of integers that would be used as input to the model\n",
    "tokenized = tokenizer(input_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e7dac",
   "metadata": {},
   "source": [
    "Both of the input sentences has been converted into a list of integers & the shape of the list is same, hence we got the results, if the length of the input string would have been different, it could not convert, hence we need to use padding, so when there are missing integers, the tokenizer can fill in placeholders to let the matrix be of same shape & work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3f10777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca4fff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128009, 128009, 128000,   3923,    374,    279,   6864,    315,   6890,\n",
      "             30],\n",
      "        [128000,   3923,    374,    279,   6864,    315,   7427,    323,   7008,\n",
      "             30]], device='mps:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "input_prompt = [\"What is the capital of India?\",\n",
    "                \"What is the capital of USA and Canada?\"]\n",
    "\n",
    "#tokeinzers convert the input string into list of integers that would be used as input to the model\n",
    "tokenized = tokenizer(input_prompt, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a9313dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745b319",
   "metadata": {},
   "source": [
    "From the above demostration, as we have used padding, & as the 2nd string is longer, the 1st string need some padding in order to fill the missing tokens, hence the EOS are addes to the 'left' because we mentioned it at the top, while we were defining tokeizer.\n",
    "we can also see that the shape of the inputs have also changed\n",
    "\n",
    "Now, lets see convert the integer back to string to understand how letter look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1340f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|eot_id|><|eot_id|><|begin_of_text|>What is the capital of India?',\n",
       " '<|begin_of_text|>What is the capital of USA and Canada?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70639046",
   "metadata": {},
   "source": [
    "Padding helped the first sentence to fill in the empty numbers, this helps to run the code faster & to a better matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5283af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128009, 128009, 128000,   3923,    374,    279,   6864,    315,   6890,\n",
       "             30],\n",
       "        [128000,   3923,    374,    279,   6864,    315,   7427,    323,   7008,\n",
       "             30]], device='mps:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99bb77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7eaee4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f75bb8",
   "metadata": {},
   "source": [
    "This attention_mask tells the model to NOT give attention to the padding words, here, as we saw, 2 EOS tokes were added to the first sentence because of padding, these will not be considered by the model. the 1's will be counted but not the 0's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ad80b",
   "metadata": {},
   "source": [
    "### Chat Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc26e3",
   "metadata": {},
   "source": [
    "Instruction Tuning: Many language models are fine tuned to follow user instructions in a chat-like format\n",
    "\n",
    "Hence, we are going to use \"apply_chat_templates()\" which converts prompt from the chat message format to a single string sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee16dd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
       "            264,   7941,  15592,  18328,    889,  21881,   1093,  42482,     13,\n",
       "         128009, 128006,    882, 128007,    271,   2940,   1587,    279,   7160,\n",
       "            743,     30, 128009, 128006,  78191, 128007,    271]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a smart AI assistant who speaks like Shakespeare.\"\n",
    "     },\n",
    "     {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"where does the sun set?\" \n",
    "     }\n",
    "]\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(prompt, \n",
    "                                          add_generation_prompt = True,\n",
    "                                          tokenize = True, #true if you want to convert to list of integers, False if you want string output\n",
    "                                          padding = True,\n",
    "                                          return_tensors = 'pt').to(device)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37552488",
   "metadata": {},
   "source": [
    "Output when the tokenize is set to \"False\":\n",
    "\n",
    "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a smart AI assistant who speaks like Shakespeare.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere does the sun set?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9e769db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a smart AI assistant who speaks like Shakespeare.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhere does the sun set?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nFair mortal, thou dost ask a query most fine,\\nConcerning the place where the sun doth']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=20)\n",
    "\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec51de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
